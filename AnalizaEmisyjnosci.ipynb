{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sekcja odczytywania danych, określania typów danych cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, cross_val_score, HalvingGridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, r2_score, mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "import timeit\n",
    "import prince\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregator rzadkich klas danej zmienej kategorycznej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RareClassAggregator(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self,  q:float = 0.1) -> None:\n",
    "        assert isinstance(q, float) and 0< q <1, \"Argument 'q' musi być liczbą zmiennoprzecinkową z przedziału (0;1)\"\n",
    "    \n",
    "        self.q: float = q\n",
    "\n",
    "\n",
    "    def validate_X(self, X:pd.DataFrame, cat_features:list[str]) -> None:\n",
    "        \"Sprawdza czy x spełnia warunki\"\n",
    "        assert isinstance(X, pd.DataFrame), \"Argument 'X' musi być instancją klasy pd.DataFrame!\"\n",
    "\n",
    "        assert isinstance(self.cat_features, list)\n",
    "\n",
    "        assert set(cat_features).issubset(self.cat_features), \"Zbyt wiele zmiennych kategorycznych przekazałeś!\"\n",
    "        assert len(cat_features) > 0, \"Przekazałeś pusty zbiór cech kategorycznych!\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X:pd.DataFrame, y: None = None, cat_features:list[str] = [] ) -> \"RareClassAggregator\":\n",
    "        \"\"\"Wylicz próg częstowliwościowy dla zmiennej kategorycznej 'cat_feature'.\n",
    "        Opis argumentów:\n",
    "        ---------\n",
    "        X:pd.DataFrame - Ramka danych typu pandas, która zawiera ceche 'cat_feature', której rzadkie cechy chcemy połączyć w jedną klase.\n",
    "        y:pd.DataFrame - Argument nieuzywany, służy do zachowania spójności transformatotów.\n",
    "        cat_features:list[str] - Lista nazw zmiennych kategorycznych.\n",
    "        \"\"\"\n",
    "\n",
    "        #'y' musi być stale równy None. Nie jest on potrzebny dla tego estimatora.\n",
    "        if y is not None:\n",
    "            raise ValueError(\"Argument 'y' musi być zawsze ustawiony na wartość None!\")\n",
    "        \n",
    "        self.X_train: pd.DataFrame = X #Zdefiniuj zbiór treningowy.\n",
    "        self.cat_features = cat_features\n",
    "        \n",
    "        self.validate_X(X = X, cat_features = self.cat_features) #Sprawx, czy argument X spełnia podstawowe założenia.\n",
    "\n",
    "\n",
    "      \n",
    "        self.freq_thresholds:dict[str, float] = {} #Tabela przechowująca wszystkie progi częstotliwościowe dla każdej cechy kategorycznej\n",
    "        self.cross_tabs: list[pd.Series] = {} #Słownik służący do przechowywania tabeli krzyżowych każdej zmiennej kategorycznej.\n",
    "\n",
    "        for cat_feature in self.cat_features:\n",
    "            class_crosstab:pd.Series = ( self.X_train[cat_feature]. #Znajdź tabelę krzyżową dla zmiennej kategorycznej 'cat_feature'\n",
    "                                         value_counts(normalize = True, sort = True))\n",
    "\n",
    "            self.freq_thresholds[cat_feature] = class_crosstab.quantile(q = self.q) #Wylicz próg częstotliwościowy dla zmiennej 'cat_feature'\n",
    "\n",
    "            self.cross_tabs[cat_feature] = class_crosstab\n",
    "\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X:pd.DataFrame, cat_features:list[str]) -> pd.DataFrame:\n",
    "        \"\"\"Transformuje ramkę danych X, agregując rzadkie klasy zmiennej kategorycznej 'cat_feature' \"\"\"\n",
    "        self.validate_X(X = X, cat_features = cat_features) #Upewnij się, że X jest ramką danych oraz, że cat_features jest NIEPUSTYM podzbiorem zbioru cech X.\n",
    "\n",
    "\n",
    "        for cat_feature in cat_features:\n",
    "            class_freqtable:pd.Series =  X[cat_feature].value_counts(normalize = True, sort = False) #Wyestymuj znormalizowaną tabelę krzyżową.\n",
    "\n",
    "\n",
    "            #Stwórz agregowaną kolumnę cechy kategorycznej.\n",
    "            aggregated_col:pd.Series =  X[cat_feature].apply(func = lambda v:   v if class_freqtable[v] >= self.freq_thresholds[cat_feature] else \"Other\").astype(dtype = \"string\")\n",
    "                                        \n",
    "            \n",
    "\n",
    "            X.loc[:, cat_feature]  = aggregated_col\n",
    "     \n",
    "        return X\n",
    "    \n",
    "\n",
    "   \n",
    "\n",
    "    def get_feature_names_out(self,) -> None:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podrasowany, zgrabny MultiOutput LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputLinearRegression(MultiOutputRegressor):\n",
    "    def __init__(self,estimator, **kwargs) -> None:\n",
    "        super().__init__(estimator = estimator,**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X:np.ndarray, y:np.ndarray):\n",
    "        y:np.ndarray[int] =  OneHotEncoder(sparse_output = False).fit_transform(X = y.reshape(-1,1))\n",
    "\n",
    "        super().fit(X = X, y = y)\n",
    "\n",
    "\n",
    "    def predict(self, X) -> np.ndarray[int]:\n",
    "        return super().predict(X = X).argmax(axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podrasowany SequentialFeatureSelector z wewnętrznym mechanizmem OHE dla zmiennych kategorycznych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedSequentialFeatureSelection(SFS):\n",
    "    def __init__(self, cat_vars_idx:list[int], num_vars_idx:list[int],estimator,  **kwargs) -> None:\n",
    "        \"\"\"\"\n",
    "        Opis argumentów: \\n\n",
    "        ---------\n",
    "        cat_vars:list[str] - Lista ciągów znaków, które identyfikują zmienne kategoryczne \\n\n",
    "\n",
    "        \"\"\"\"\"\"\"\"\"\n",
    "        super().__init__(estimator = estimator,**kwargs)\n",
    "        \n",
    "        self.cat_vars_idx:list[int] = cat_vars_idx #Indeksy wszystkich zmiennych kategorycznych przyszłej ramki danych X.\n",
    "        self.num_vars_idx:list[int] = num_vars_idx #Indeksy wszystkich zmiennych numerycznych przyszłej ramki danych X.\n",
    "\n",
    "    \n",
    "\n",
    "    def fit(self, X:np.ndarray, y:np.ndarray):\n",
    "        super().fit(X = X, y =y)\n",
    "\n",
    "\n",
    "\n",
    "        self.candidates_idx:list[int] = np.flatnonzero(a = self.support_) #Tablica indeksów zmiennych optymalnych.\n",
    "\n",
    "        self.cat_candidates_idx:list[int] = np.intersect1d(ar1 = self.candidates_idx, ar2 = self.cat_vars_idx) #Tablica indeksów zmiennych kategorycznych optymalnych.\n",
    "        self.noncat_candidates_idx:list[int] = np.setdiff1d(ar1 = self.candidates_idx, ar2 = self.cat_candidates_idx) #Tablica indeksów zmiennych numerycznych optymalnych.\n",
    "\n",
    "\n",
    "      \n",
    "        self.pre_predictive_transformer = ColumnTransformer(transformers = [(\"OHE\", OneHotEncoder(sparse_output = False), self.cat_candidates_idx),\n",
    "                                                                            (\"Identity\", FunctionTransformer(), self.noncat_candidates_idx)],\n",
    "                                                                            remainder = \"drop\")\n",
    "\n",
    "        self.X_train = self.pre_predictive_transformer.fit_transform(X = X)\n",
    "        self.y_train = y.copy()\n",
    "\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    \n",
    "    def _get_best_new_feature_score(self, estimator, X:np.ndarray, y:np.ndarray, cv:int, current_mask):\n",
    "        # Return the best new feature and its score to add to the current_mask,\n",
    "        # i.e. return the best new feature and its score to add (resp. remove)\n",
    "        # when doing forward selection (resp. backward selection).\n",
    "        # Feature will be added if the current score and past score are greater\n",
    "        # than tol when n_feature is auto,\n",
    "      \n",
    "\n",
    "        candidate_feature_indices = np.flatnonzero(~current_mask) #Indeksy zmiennych, które nie zostały jeszcze wybrane.\n",
    "        scores = {}\n",
    "        \n",
    "    \n",
    "        for feature_idx in candidate_feature_indices:\n",
    "            candidate_mask:np.ndarray[bool] = current_mask.copy() #Tworzymy kopię tablicy maskowej cech, które już wybraliśmy.\n",
    "            candidate_mask[feature_idx] = True #W miejsce feature_indx wstawiamy True. Udajemy, że wybraliśmy cechę feature_indx.\n",
    "\n",
    "            candidates:np.ndarray[int] = np.flatnonzero(a = candidate_mask) #Znajdź bezwzględne indeksy zmiennych kandydatowych.\n",
    "\n",
    "            cat_candidates_idx:np.ndarray[int] = np.intersect1d(ar1 = candidates, ar2 = self.cat_vars_idx) #Znajdż bezwzglęne indeksy zmiennych kategorycznych kandydatów.\n",
    "            noCat_candidates_idx:np.ndarray[int] = np.setdiff1d(ar1 = candidates, ar2 = cat_candidates_idx)\n",
    "\n",
    "  \n",
    "\n",
    "            #Kodowator OHE, który przekształca na gorąco zmienne kategoryczne, a pozostałych zmiennych nie narusza.\n",
    "            candidates_transformer = ColumnTransformer(transformers = [(\"OHE\", OneHotEncoder(sparse_output = False), cat_candidates_idx),\n",
    "                                                                       (\"Identity\", FunctionTransformer(), noCat_candidates_idx)],\n",
    "                                                                         remainder = \"drop\")\n",
    "                                                                       \n",
    "                    \n",
    "            X_new = candidates_transformer.fit_transform(X = X)\n",
    "\n",
    "        \n",
    "            scores[feature_idx] = cross_val_score(\n",
    "                estimator,\n",
    "                X_new,\n",
    "                y,\n",
    "                cv=cv,\n",
    "                scoring=self.scoring,\n",
    "                n_jobs=self.n_jobs,\n",
    "            ).mean()\n",
    "\n",
    "\n",
    "        new_feature_idx = max(scores, key=lambda feature_idx: scores[feature_idx])\n",
    "\n",
    "        \n",
    "        return new_feature_idx, scores[new_feature_idx]\n",
    "    \n",
    "\n",
    "    def predict(self, X:np.ndarray):\n",
    "        #X to jest oryginalna ramka danych, która zawiera cechy wszystkie, w tym cechy optymalne.\n",
    "        self.estimator.fit(X = self.X_train, y = self.y_train) #X_train to jest  treningowa ramka danych z optymalnymi predyktorami\n",
    "\n",
    "\n",
    "        #Kodowator OHE, który przekształca na gorąco zmienne kategoryczne, a pozostałych zmiennych nie narusza.\n",
    "        candidates_transformer = ColumnTransformer(transformers = [(\"OHE\", OneHotEncoder(sparse_output = False), self.cat_candidates_idx)\n",
    "                                                                   ,(\"Identity\", FunctionTransformer(), self.noncat_candidates_idx)], \n",
    "                                    remainder = \"drop\")\n",
    "\n",
    "\n",
    "\n",
    "        X_new:np.ndarray = candidates_transformer.fit_transform(X = X)\n",
    "   \n",
    "\n",
    "        return self.estimator.predict(X = X_new)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie metody porównaczej.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "class ModelsComparisom():\n",
    "    def __init__(self,  Filename:str, target_var:str, dtypes:dict[str, \"datatype\"], Models:dict[str, \"estimator\"], Models_hipparams:dict[str, dict],\n",
    "                 n_splits:int = 5, train_size:float = 0.8, test_size:float = 0.2, bins:list[int] = [150, 250], show_plots:bool = True) -> None:\n",
    "        \"\"\"Konstruktor klasy, która trenuje modele. \n",
    "        Opis argumentów konstruktora:\n",
    "        ---------\n",
    "        Filename: str - Nazwa pliku, w której zawarta jest ramka danych typu pandas, która przechowuje zmienne objaśniające i zmienną objaśnianą. \\n\n",
    "        target_var:str - Nazwa zmiennej objaśnianej. \\n\n",
    "\n",
    "        dtypes:dict[str, \"datatype\"] - Typ danych każdej kolumny w pliku. \\n\n",
    "\n",
    "        Models:dict[str, \"Estimator\"] - Słownik, którego wartościami są instancje modeli, które chcemy wytrenować. Kluczami są umowne nazwy modeli. \\n\n",
    "        Models_hipparams:dict[str, dict] - Słownik, którego kluczami są umowne nazwy modeli, a którego wartościami są siatki parametrów danego modelu. \\n\n",
    "\n",
    "        n_splits:int - Liczba podziałów na zbiór treningowy i testowy. \\n\n",
    "        train_size:float - Odsetek obserwacji treningowych. \\n\n",
    "        test_size:float - Odsetek obserwacji testowych. \\n\n",
    "\n",
    "        bins:list[int]  - Wartości brzegowe dla dyskretyzacji zmiennej docelowej.\n",
    "        show_plots: bool - Czy chcesz, aby pokazywano wykresy.\n",
    "\n",
    "        ---------\n",
    "        \"\"\"\n",
    "     \n",
    "        self.dtypes:dict[str, \"datatype\"] = dtypes\n",
    "\n",
    "        self.Dataset:pd.DataFrame = self.read_dataframe(filename = Filename, sep = ';', dec_sep = ',') \n",
    "        self.features:list[str] = self.Dataset.columns\n",
    " \n",
    "        \n",
    "        self.Cat_features:list[str] = [feature for feature in self.features if dtypes[feature] == \"string\"]\n",
    "        self.Num_features:list[str] =  list(self.Dataset.select_dtypes(include = \"number\").columns)\n",
    "    \n",
    "     \n",
    "        self.target_var:str = target_var\n",
    "        self.bins:list[int] = bins\n",
    "\n",
    "        self.Cat_predictors:list[str] = []\n",
    "        self.Num_predictors:list[str] = []\n",
    "        self.PCA_predictors:list[str] = []\n",
    "\n",
    "        self.Models:dict[str, \"estimator\"] = Models\n",
    "        self.Models_hipparams:dict[str, dict] = Models_hipparams\n",
    "\n",
    "        self.n_splits:int = n_splits\n",
    "        self.train_size:float = train_size\n",
    "        self.test_size:float = test_size\n",
    "\n",
    "        self.show_plots:bool = show_plots\n",
    "\n",
    "\n",
    "        self.create_predictions_dataframe()\n",
    "        self.model_names:list[str] = list(self.Models.keys())\n",
    "    \n",
    "        \n",
    "\n",
    "    def read_dataframe(self, filename:str, \n",
    "                      sep:str =';', dec_sep:str =',') -> pd.DataFrame:\n",
    "        \"\"\"Odczytuje plik o nazwie filename i wprowadza dane z pliku do ramki danych.\"\"\"\n",
    "     \n",
    "    \n",
    "        Dataset:pd.DataFrame = pd.read_csv(filename,\n",
    "                            sep=sep, dtype = self.dtypes, decimal = dec_sep) #Wczytaj plik z danymi.\n",
    "\n",
    "\n",
    "        return Dataset\n",
    "    \n",
    "\n",
    "\n",
    "    def plot_bar_plot(self, FreqTable:pd.Series, cat_feature:str, Showxlabels:bool = False) -> None:\n",
    "        \"\"\"Funkcja rysuje wykres słupkowy na bazie tabeli kontygancji. \n",
    "        1) FreqTable  - Tabela częstotliwości kategori danej zmiennej kategorycznej\n",
    "        2) CatFeature  - Cecha kategoryczna, której histogram chcemy narysować.\n",
    "        3) Showxlabel - Zmienna typu bool. Jeżeli ustawiona na True, to etykietki osi Ox są wyświetlane.\"\"\"\n",
    "        \n",
    "        plt.figure(figsize = (10,5)) #Stwórz płótno, na którym  będzie rysowany wykres\n",
    "    \n",
    "        axes = sns.barplot(x = FreqTable.index, y = FreqTable.values)\n",
    "\n",
    "\n",
    "        axes.set_ylabel(f\"Częstość klasy\") #Ustaw etykietke pionowej osi.\n",
    "\n",
    "        axes.set_xticklabels([]) #Usuń etykiety tyknięć na osi Ox.\n",
    "        axes.spines[[\"top\", \"right\"]].set_visible(False)\n",
    "\n",
    "        axes.set_title(f\"Histogram klas cechy {cat_feature}\") #Ustaw tytuł wykresu.\n",
    "\n",
    "        axes.set_ylim(0, 1.05*np.max(FreqTable))\n",
    "\n",
    "        if Showxlabels == True:\n",
    "            axes.set_xticklabels(labels = FreqTable.index)\n",
    "            \n",
    "\n",
    "\n",
    "   \n",
    "    def compute_and_draw_correlation_matrix(self,) -> None:\n",
    "        \"\"\"Funkcja wylicza macierz korelaji dla zmiennych z listy FloatFeatures. Ponadto, rysuje tę macierz korelacji na wykresie, aby można\n",
    "        było sobie uzmysłowić relacje między zmiennymi\"\"\"\n",
    "\n",
    "        CorrMatrix:pd.DataFrame =  self.Dataset[self.Num_features].corr(method = \"pearson\")\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        sns.heatmap(CorrMatrix, annot=True, cmap='magma', vmin=-1, vmax=1)\n",
    "        plt.title('Macierz korelacji dla zmiennych ciągłych')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def delete_quasiid_feature(self) -> None:\n",
    "        \"\"\"Funkcja usuwa quasi-identyfikator zmienną oraz jedną obserwację, która zawiera klasę, która występuje tylko raz.\"\"\"\n",
    "\n",
    "        self.Dataset.drop(columns = [\"Model\"], inplace = True)\n",
    "\n",
    "        self.Dataset = self.Dataset.loc[self.Dataset[\"Fuel Type\"]!=\"Other\", :].copy()\n",
    "\n",
    "    \n",
    "\n",
    "    def plot_KDE(self, Condition:str | None = None) -> None:\n",
    "        \"\"\"Ta funkcja rysuje wykresy gęstości prawdopodobieństwa dla zmiennych ciągłych\"\"\"\n",
    "\n",
    "        for float_feature in self.Num_features:\n",
    "            figure = plt.figure(num = f\"KDE_plot_{float_feature}\")\n",
    "            axes = figure.add_subplot()\n",
    "\n",
    "            sns.kdeplot(data = self.Dataset, x = float_feature, ax = axes, hue = Condition)\n",
    "            axes.set_title(f\"Wykres gęstości prawdopodobieństwa dla zmiennej {float_feature}\")\n",
    "\n",
    "\n",
    "\n",
    "    def plot_boxplot(self, Condition: str | None = None) -> None:\n",
    "        \"\"\"Ta funkcja rysuje wykresy pudełkowe dla zmiennych ciągłych.\n",
    "        1) Dataset - oryginalny zestaw danych.\n",
    "        2) FloatFeatures - zmienne numeryczne\"\"\"\n",
    "\n",
    "        for float_feature in self.Num_features:\n",
    "            figure = plt.figure(num = f\"BOX_plot_{float_feature}\")\n",
    "            axes = figure.add_subplot()\n",
    "\n",
    "            sns.boxplot(self.Dataset, x = float_feature, hue = Condition)\n",
    "\n",
    "            axes.set_title(f\"Wykres pudełkowy dla zmiennej {float_feature}\")\n",
    "\n",
    "\n",
    "    def plot_violinplot(self,Condition:str | None = None) -> None:\n",
    "        \"\"\"Ta funkcja rysuje wykresy skrzypcowe dla zmiennych ciągłych.\n",
    "        1) Dataset - oryginalny zestaw danych.\n",
    "        2) FloatFeatures - zmienne numeryczne\n",
    "        3) Condition = Pewna zmienna kategoryczna, za pomocą której stworzą się warunkowe wykresy skrzypcowe ze względu przynależność do klasy.\"\"\"\n",
    "\n",
    "        for float_feature in self.Num_features:\n",
    "            figure = plt.figure(num = f\"VIOLIN_plot_{float_feature}\")\n",
    "            axes = figure.add_subplot()\n",
    "\n",
    "            sns.violinplot(self.Dataset, x = float_feature, hue = Condition)\n",
    "\n",
    "            axes.set_title(f\"Wykres skrzypcowy dla zmiennej {float_feature}\")\n",
    "\n",
    "            axes.legend([])\n",
    "\n",
    "            axes.grid(True, alpha = 0.6)\n",
    "            axes.spines[['top','right']].set_visible(False)\n",
    "\n",
    "\n",
    "    def plot_pairplot(self,   Condition: str | None = None) -> None:\n",
    "        \"\"\"Funkcja rysuje wykres parowy dla wszystkich par zmiennych ciągłych.\n",
    "        \"\"\"\n",
    "        sns.pairplot(self.Dataset, hue = Condition ,diag_kind = \"kde\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def discretize(self) -> None:\n",
    "        \"\"\"Discretize the target variable with respect to given bins\"\"\"\n",
    "        labels:list[int] = [i for i in range(len(self.bins) -1)] #Find the list of integer-labels.\n",
    "     \n",
    "\n",
    "        discretized_feature:pd.Series = pd.cut(x = self.Dataset[self.target_var],  #Finally, discretize the labels.\n",
    "                                    bins = self.bins, \n",
    "                                    labels = labels)\n",
    "        \n",
    "\n",
    "        self.Dataset[self.target_var_discr] = discretized_feature #Add brand-new discretized feature to the dataset.\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def descriptive_statistics(self) -> None:\n",
    "        \"\"\"\"This  methods shows some statistical properties of the features of the dataframe. In this section we also examine the discriminant-ability of the variables.\n",
    "        In other words, we're manually looking for the most optimal candidates for predictors\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Agregacja rzadkich klas.\n",
    "        RareClassAggregator_inst = RareClassAggregator(q = 0.15) #Zdefiniuj obiekt klasy RareClassAggregator, który będzie agregował rzadkie klasy każdej cechy.\n",
    "\n",
    "        RareClassAggregator_inst.fit(X = self.Dataset, y = None,  #Znajdź odpowiednie parametry  estymatora.\n",
    "                                     cat_features = self.Cat_features)\n",
    "\n",
    "        self.Dataset:pd.DataFrame = RareClassAggregator_inst.transform(X = self.Dataset, cat_features = self.Cat_features) #Przekształć obecny zbiór danych.\n",
    "\n",
    "        cross_tabs:list[pd.Series] = RareClassAggregator_inst.cross_tabs #To jest lista zawierająca tablice krzyżowe każdej cechy\n",
    "\n",
    "        for cat_feature in self.Cat_features:\n",
    "            cros_tab:pd.Series = cross_tabs[cat_feature]\n",
    "\n",
    "\n",
    "            if self.show_plots is True:\n",
    "                self.plot_bar_plot(FreqTable = cros_tab, cat_feature = cat_feature, Showxlabels = False)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "        #Kasowanie zbędnej obserwacji oraz quasi-id kolumny\n",
    "        self.delete_quasiid_feature()\n",
    "\n",
    "        #Nadaj zdyskretyzowanej zmiennej docelowej nazwę.\n",
    "        self.target_var_discr = self.target_var +\"_disc\"\n",
    "\n",
    "        #Zdyskretyzuj zmienną objaśnianą.\n",
    "        self.discretize()\n",
    "\n",
    "        \n",
    "        #Narysuj wykresy charakteryzujące (relacje między zmiennymi) oraz (charakterystyki zmiennych).\n",
    "        #RYSOWANIE MACIERZY KORELACJI DLA ZMIENNYCH NUMERYCZNYCH\n",
    "\n",
    "        target_var_discr_histogram:pd.Series = self.Dataset[self.target_var_discr].value_counts(normalize = True, sort = False)\n",
    "\n",
    "        if self.show_plots is True:\n",
    "            self.compute_and_draw_correlation_matrix()\n",
    "\n",
    "            #RYSOWANIE WYKRESÓW SKRZYPCOWYCH DLA ZMIENNYCH NUMERYCZNYCH\n",
    "            self.plot_violinplot()\n",
    "\n",
    "            #Rysowanie wykresu parowego dla zmiennych numerycznych\n",
    "            self.plot_pairplot()\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            self.plot_bar_plot(FreqTable = target_var_discr_histogram,   #Narysuj wykres słupkowy częstotliwości dla zmiennej celu zdyskretyzowanej.\n",
    "                            cat_feature =  self.target_var_discr, \n",
    "                            Showxlabels = False)\n",
    "            \n",
    "        \n",
    "            self.plot_KDE(Condition = self.target_var_discr)  #Wykresy gęstości warunkowe\n",
    "            self.plot_bar_plot(Condition = self.target_var_discr) #Wykresy pudełkowe warunkowe\n",
    "\n",
    "        \n",
    "            self.plot_pairplot(Condition = self.target_var_discr)\n",
    "        \n",
    "\n",
    "        #Ustal ostateczny zbiór predyktorów.\n",
    "        self.predictors:list[str] = ['Make', \"Vehicle Class\",'Engine Size(L)','Cylinders','Transmission','Fuel Type',\"Fuel Consumption City (L/100 km)\", \"Fuel Consumption Hwy (L/100 km)\",\n",
    "                                                                                       \"Fuel Consumption Comb (L/100 km)\",\"Fuel Consumption Comb (mpg)\"]\n",
    "      \n",
    "        #Podziel zbiór predyktorów na zmienne numeryczne oraz zmienne kategoryczne odpowiednio.\n",
    "        self.num_predictors_idx:list[int] = []\n",
    "        self.cat_predictors_idx:list[int] = []\n",
    "\n",
    "        \n",
    "        self.PCA_predictors_idx:list[int] = []\n",
    "       \n",
    "\n",
    "        for idx, feature in enumerate(self.predictors):\n",
    "            if self.dtypes[feature] == \"string\": #Sprawdzanie, czy zmienna jest zmienną kategoryczną.\n",
    "                self.cat_predictors_idx.append(idx) #Dodaj indeks zmiennej kategorycznej do listy zmiennych kategorycznych.\n",
    "\n",
    "            elif self.dtypes[feature] in [np.float32, np.float64, np.int16,np.int32, np.int64]: #Sprawdzanie, czy zmienna jest typu numerycznego.\n",
    "                self.num_predictors_idx.append(idx) #Dodaj indeks zmiennej numerycznej do listy zmiennych numerycznych.\n",
    "\n",
    "                if feature.startswith(\"Fuel\"): #Znajdź zmienną, które podlegają PCA.\n",
    "                    self.PCA_predictors_idx.append(idx)\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "    # def PorównajCzasyTrenowania(self, type:str = \"bez strojenia\"):\n",
    "    #     \"\"\"Ta metoda, dla ustalonego, jednego z trzech, trybów trenowania, wyświetla czasy trenowania różncych algorytmów klasyfikujących.\"\"\"\n",
    "\n",
    "    #     x_axis_values:list[int] = list(range(1, self.n_splits+1)) #Wartości na osi Ox.\n",
    "\n",
    "    #     figure  = plt.figure(num = f\"Porównanie czasów trenowania, wersja {type}\") #Zdefiniuj okno.\n",
    "    #     axes = figure.add_subplot() #Zdefiniuj osie.\n",
    "\n",
    "\n",
    "    #     if type == \"bez strojenia\":\n",
    "    #         for model_name in self.Models.keys():\n",
    "    #             axes.plot(x_axis_values, self.Untuned_train_time[model_name])\n",
    "\n",
    "    #     elif type == \"ze strojeniem\":\n",
    "    #         for model_name in self.Models.keys():\n",
    "    #             axes.plot(x_axis_values,  self.Tuned_train_time[model_name])\n",
    "\n",
    "    #     elif type == \"z wyborem cech\":\n",
    "    #         for model_name in self.Models.keys():\n",
    "    #             axes.plot(x_axis_values, self.FS_train_time[model_name])\n",
    "    #     else:\n",
    "    #         raise ValueError(\"Błędny tryb trenowania\")\n",
    "\n",
    "\n",
    "    #     axes.legend(list(self.Models.keys()))\n",
    "    #     axes.grid(True)\n",
    "    #     axes.spines[[\"top\", \"right\"]].set_visible(False)\n",
    "\n",
    "    #     axes.set_xlabel(\"Numer podziału\")\n",
    "    #     axes.set_ylabel('Czas tren.')\n",
    "    #     axes.set_title(f\"Czas trenowania modeli w milisekundach, wersja {type}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "  \n",
    "    def create_predictions_dataframe(self,) -> None:\n",
    "        \"\"\"Creates a dataframe for storing the actual labels and labels predicted by each of the model. The column-system of the dataframe is four-level. The syntax for selecting a column is as follows:\n",
    "        FactVsPrediction[('model_name', 'train_type', 'iter_idx, 'y_type')] \\n\n",
    "        'model_name' is a name of the model,  \\n\n",
    "        'train_type' is a type of training:  noFS_untuned, noFS_tuned, FS_untuned, FS_tuned, \\n\n",
    "        'iter_idx' is an index of the main iteration, \\n\n",
    "        'y_type' is a type of y labels: actual or predicted. \\n\n",
    "\n",
    "         \"\"\"\n",
    "        self.training_types: list[str] = [\"noFS_untuned\", \"noFS_tuned\", \n",
    "                                          \"FS_untuned\",\"FS_tuned\"\n",
    "                                          ,\"FE\"]\n",
    "\n",
    "        Indeces = pd.MultiIndex.from_product( [list(self.Models.keys()),self.training_types, range(self.n_splits), [\"True\", \"Pred\"] ] #Stwórz  hierarchiczny system indeksów dla kolumn.\n",
    "                                            ,names = [\"model\",\"train_type\" ,\"iter_idx\", \"y_type\"]) #Nadaj poszczególnym poziomom wyjaśnialne i sensowne nazwy.\n",
    "        \n",
    "\n",
    "        self.FactVsPrediction:pd.DataFrame =  pd.DataFrame(data = None,  columns = Indeces, dtype = np.int16)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # def StworzRamkeCzasowa(self, ) -> pd.DataFrame:\n",
    "    #     \"\"\"\"Funkcja tworzy ramkę danych typu pandas, której wartości za indeksowane za pomocą pary (model_name, split_indx).\n",
    "    #     W komórce (model_name, split_indx) znajduje się czas wytrenowania modelu model_name w podziale nr split_indx\n",
    "    #     \"\"\"\n",
    "        \n",
    "    #     Col_indeces:list[str] = list(self.Models.keys())\n",
    "    #     Row_indeces:list[int] = list(range(self.n_splits))\n",
    "\n",
    "    #     return pd.DataFrame(columns = Col_indeces, index = Row_indeces, dtype = np.float64)\n",
    "    \n",
    "\n",
    "    # def PrzygotujRamkiCzasowe(self, ) -> None:\n",
    "    #     \"\"\"Metoda definiuje trzy ramki czasowe, w których będą przechowywane czasy dopasowania modeli strojonych i niestrojonych\"\"\"\n",
    "    #     self.Untuned_train_time = self.StworzRamkeCzasowa()\n",
    "    #     self.Tuned_train_time = self.StworzRamkeCzasowa()\n",
    "    #     self.FS_train_time = self.StworzRamkeCzasowa()\n",
    "\n",
    "    \n",
    "\n",
    "    def transform_predictors(self,  num_predictors_idx:list[int], PCA_predictors_idx:list[int] | None = None, cat_predictors_idx:list[int] | None = None, train_type: str = \"noFS\") ->  ColumnTransformer:\n",
    "        \"\"\"Transform the predictors. The exact  list of transformers is dependent on whether FS is included or not.\n",
    "        If not (featsel = False), the following transformations are being applied: OrdinalEncoding, StandardScaler, PCA\n",
    "        \n",
    "        Parameters \\n\n",
    "        --------- \\n\n",
    "\n",
    "        num_predictors_idx : list[int] : list of indeces of numerical variables. \\n\n",
    "        PCA_predictors_idx : list[int] : list of indeces of PCA variables. \\n\n",
    "        cat_predictors_idx : list[int] : list of indeces of categorical variables.  \\n\n",
    "        featsel : str  = \"noFS\" : type of learning: noFS (noFeatureSelection), FS (FeatureSelection), FE (FeatureExtraction)\n",
    "\n",
    "        ---------\n",
    "\n",
    "        Returns: \\n\n",
    "        ColumnTransformer\n",
    "\n",
    "        \"\"\"\n",
    "        doPCA: bool = train_type == \"noFS\" or train_type == \"FS\"\n",
    "\n",
    "        if doPCA:\n",
    "            noPCA_predictors_idx:list[int] = np.setdiff1d(ar1 = num_predictors_idx, ar2 = PCA_predictors_idx) #Find the NUMERICAL predictors which won't be PCA-transformed.\n",
    "            noPCA_transformer = Pipeline(steps = [(\"Scaler\", StandardScaler())]) #Define transformer for noPCA_predictors.\n",
    "        \n",
    "\n",
    "                                                \n",
    "            PCA_transformer = Pipeline(steps = [(\"Scaler\", StandardScaler()),   (\"PCA\", PCA(n_components =0.9))      ]) #Define transformer for PCA_predictors.\n",
    "        \n",
    "\n",
    "      \n",
    "        if train_type == \"noFS\": #If  we're not including FeatureSelection, we directly encoding categorical variables using OneHotEncoder.\n",
    "            Predictors_transformer = ColumnTransformer(transformers = [(\"OHE\", OneHotEncoder(sparse_output = False), cat_predictors_idx),\n",
    "                                                                            (\"Numerical\", noPCA_transformer, noPCA_predictors_idx),\n",
    "                                                                            (\"PCA\", PCA_transformer, PCA_predictors_idx)],\n",
    "                                                                            remainder = \"passthrough\"\n",
    "                                                                        )\n",
    "            \n",
    "        elif train_type == \"FS\": #However, if we are including FeatureSelection, first we encode categorical variables using OrdinalEncoder. OneHotEncoder for this type of training will be applied inside WrappedFeatureSelection class.\n",
    "        \n",
    "            Predictors_transformer = ColumnTransformer(transformers = [(\"ORD\", OrdinalEncoder(), cat_predictors_idx),\n",
    "                                                                            (\"Numerical\", noPCA_transformer, noPCA_predictors_idx),\n",
    "                                                                            (\"PCA\", PCA_transformer, PCA_predictors_idx)],\n",
    "                                                                            remainder = \"passthrough\"\n",
    "                                                                      )\n",
    "            Predictors_transformer.set_output(transform = \"pandas\") #Set the output type to output\n",
    "\n",
    "        elif train_type == \"FE\":\n",
    "            Predictors_transformer = ColumnTransformer(transformers = [(\"Scaler\", StandardScaler(), num_predictors_idx)], remainder = \"passthrough\")\n",
    "            Predictors_transformer.set_output(transform = \"pandas\")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type of training (train_type) was passed \")\n",
    "\n",
    "\n",
    "                \n",
    "        \n",
    "\n",
    "        return Predictors_transformer\n",
    "\n",
    "\n",
    "    def train_with_FS(self, train_indx:np.ndarray, test_indx:np.ndarray,split_indx:int = 0) -> None:\n",
    "        \"\"\"Training  the machine learning models with SequentialFeatureSelection included. \\n\n",
    "\n",
    "        Parameters:\n",
    "        --------- \n",
    "        train_indx : np.ndarray : A numpy ndarray of training indeces. \\n\n",
    "        test_indx : np.ndarray :  A numpy ndarray of testing indices . \\n\n",
    "        split_indx : int : Split indicator. \\n\n",
    "        --------\n",
    "\n",
    "        Returns: \n",
    "        None\n",
    "        \"\"\"\n",
    "        X_train:np.ndarray = self.X[train_indx, :] #Training set of possible predictors.\n",
    "        X_test:np.ndarray = self.X[test_indx, :] #Testing set of possible predictors.\n",
    "\n",
    "        y_train:np.ndarray = self.y[train_indx] #Training set of target variable.\n",
    "        y_test:np.ndarray = self.y[test_indx] #Testing set of target variable\n",
    "        \n",
    "\n",
    "        predictors_transformer = self.transform_predictors(num_predictors_idx  = self.num_predictors_idx, #Define the predictors preprocessing transformer.\n",
    "                                                             PCA_predictors_idx= self.PCA_predictors_idx,\n",
    "                                                             cat_predictors_idx=self.cat_predictors_idx, \n",
    "                                                             train_type = \"FS\")\n",
    "        predictors_transformer.fit(X = X_train) #Fit the transformer using TRAINING DATA to avoid data-leakage problem.\n",
    "\n",
    "                                                        \n",
    "        X_train:pd.DataFrame = predictors_transformer.transform(X = X_train) #Transform the training set X with the transformer.\n",
    "        X_test:pd.DataFrame = predictors_transformer.transform(X = X_test) #Transform the testing set X with the transformer.\n",
    "\n",
    "        #It's worth noting the dimensionality of the transformed training and testing set may change due to PCA, which is a reduction method.\n",
    "        #Because of that, the relatives positions of variables may change. The following code finds the new indeces of both categorical and numerical variables.\n",
    "        cat_vars_idx:list[int] = [] #An array of categorical variables' indces.\n",
    "        num_vars_idx:list[int] = [] #An array of numerical variables' indeces.\n",
    "\n",
    "        for i, var in enumerate(X_train.columns):\n",
    "            if var.startswith(\"ORD\"): #How can we recognize categorical variable? Well, it's been  transformed using OrdinalEncoder which adds prefix \"ORD\" to the variable' name.\n",
    "                cat_vars_idx.append(i) #Add the index.\n",
    "            \n",
    "                                        \n",
    "\n",
    "            else: #Otherwise it's a numerical predictor.\n",
    "                num_vars_idx.append(i)\n",
    "\n",
    "\n",
    "\n",
    "        for model_name in self.Models.keys():\n",
    "            model  = self.Models[model_name] #The machine learning model we're training.\n",
    "\n",
    "            model_paramgrid = self.Models_hipparams[model_name] #The parameters_space for the model.\n",
    "            trans_model_paramgrid = {f\"Model__{param}\":model_paramgrid[param] for param in  model_paramgrid.keys()} #Adjust the names of the hyperparameters. Why? Because de facto we're tuning SeqFeatSel and we wanna tune the model.\n",
    "          \n",
    "            \n",
    "            scoring = make_scorer(precision_score, average = \"weighted\")   #Define the scoring function for both SFS and GridSearch              \n",
    "\n",
    "\n",
    "            SFS_inst = WrappedSequentialFeatureSelection(cat_vars_idx = cat_vars_idx, num_vars_idx = num_vars_idx,\n",
    "                                              estimator = model, \n",
    "                                              n_features_to_select = 4, n_jobs = -1, cv = 2, scoring = scoring)\n",
    "            \n",
    "            model_FS_tuned: Pipeline = Pipeline(steps = [(\"FeatSel\", SFS_inst), (\"Model\", model)])\n",
    "        \n",
    "   \n",
    "     \n",
    "            GridSearch = GridSearchCV(estimator =model_FS_tuned , param_grid = trans_model_paramgrid,   #Define the GridSearch.\n",
    "                                      n_jobs = -1, scoring = scoring, cv = 2) \n",
    "            GridSearch.fit(X = X_train, y = y_train) #Train the GridSearch with training data.\n",
    "    \n",
    "\n",
    "            y_pred_tuned:np.ndarray = GridSearch.best_estimator_.predict(X = X_test) #Predict the labels using the FS_tuned model.\n",
    "\n",
    "\n",
    "\n",
    "            SFS_inst.fit(X = X_train, y = y_train) #Fit the model FS_notuned.\n",
    "            y_pred_untuned:np.ndarray = SFS_inst.predict(X = X_test) #Predict the labels with FS_notuned model.\n",
    "\n",
    "\n",
    "\n",
    "            #Save the  both actual and predictes labels  for both FS_tuned and FS_untuned models.\n",
    "\n",
    "            self.FactVsPrediction[(model_name, \"FS_tuned\", split_indx, \"True\")] = y_test\n",
    "            self.FactVsPrediction[(model_name, \"FS_tuned\", split_indx, \"Pred\")] = y_pred_tuned\n",
    "\n",
    "            self.FactVsPrediction[(model_name,\"FS_untuned\", split_indx, \"True\")] = y_test\n",
    "            self.FactVsPrediction[(model_name,\"FS_untuned\",split_indx, \"Pred\")] = y_pred_untuned\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train_without_FS(self, train_indx:np.ndarray, test_indx:np.ndarray, split_indx:int = 0) -> None:\n",
    "        \"\"\"Train the ML models without FeatureSelection.\n",
    "        \n",
    "        Parameters:\n",
    "        --------- \n",
    "        train_indx : np.ndarray : A numpy ndarray of training indeces. \\n\n",
    "        test_indx : np.ndarray :  A numpy ndarray of testing indices . \\n\n",
    "        split_indx : int : Split indicator. \\n\n",
    "        --------\n",
    "\n",
    "        Returns: \n",
    "        None\n",
    "\n",
    "        \"\"\"\n",
    "        predictors_transformer:ColumnTransformer = self.transform_predictors( num_predictors_idx = self.num_predictors_idx, \n",
    "                                                                        PCA_predictors_idx = self.PCA_predictors_idx, \n",
    "                                                                        cat_predictors_idx = self.cat_predictors_idx,\n",
    "                                                                        train_type = \"noFS\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        X_train:np.ndarray = self.X[train_indx, :] #Treningowy zbiór predyktorów.\n",
    "        X_test:np.ndarray = self.X[test_indx, :] #Testowy zbiór predyktorów.\n",
    "\n",
    "        y_train: np.ndarray = self.y[train_indx]\n",
    "        y_test: np.ndarray = self.y[test_indx]\n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        for model_name in self.Models.keys():\n",
    "            model: 'estimator' = self.Models[model_name] #Instancja danego modelu.\n",
    "\n",
    "            model_paramgrid: dict[str, dict] = self.Models_hipparams[model_name] #Siatka hiperparametrów modelu.\n",
    "\n",
    "\n",
    "            trans_model = Pipeline(steps = [(\"Transformations\", predictors_transformer), (\"Classifier\",model)]\n",
    "                                       )\n",
    "                 \n",
    "            #Trenowanie modeli bez strojenia hiperparametrów.\n",
    "            scoring = make_scorer(precision_score, average = \"weighted\")\n",
    "\n",
    "     \n",
    "            trans_model.fit(X = X_train, y = y_train)\n",
    "\n",
    "\n",
    "            y_pred:np.ndarray = trans_model.predict(X = X_test)\n",
    "\n",
    "      \n",
    "\n",
    "            self.FactVsPrediction[(model_name, \"noFS_untuned\", split_indx, \"True\")] = y_test\n",
    "            self.FactVsPrediction[(model_name, \"noFS_untuned\", split_indx, \"Pred\")] = y_pred\n",
    "        \n",
    "\n",
    "\n",
    "            #Trenowanie modeli ze strojeniem hiperparametrów.\n",
    "            trans_model_paramgrid = {f\"Classifier__{param}\":model_paramgrid[param] for param in  model_paramgrid.keys()} #Dopasuj nazwy hiperparametrów do estymatora, \n",
    "                                                                                         # który nie jest bezpośrednio klasyfikatorem.\n",
    "          \n",
    "            #GridSearch = HalvingGridSearchCV(estimator = trans_model, param_grid = trans_model_paramgrid, n_jobs = -1, factor = 2, cv = 5, scoring = scoring)\n",
    "            GridSearch = GridSearchCV(estimator = trans_model, param_grid = trans_model_paramgrid, n_jobs = -1, scoring = scoring, cv = 2)\n",
    "          \n",
    "            \n",
    "            GridSearch.fit(X = X_train, y = y_train)\n",
    "            y_pred:np.ndarray = GridSearch.predict(X = X_test)\n",
    "\n",
    "\n",
    "            self.FactVsPrediction[(model_name, \"noFS_tuned\", split_indx, \"True\")] = y_test\n",
    "            self.FactVsPrediction[(model_name, \"noFS_tuned\", split_indx, \"Pred\")] = y_pred\n",
    "\n",
    "    from prince import FAMD\n",
    "\n",
    "\n",
    "    def train_with_FE(self, train_idx:np.ndarray, test_idx:np.ndarray, split_idx:int) -> None:\n",
    "        \"\"\"Perfoms training the machine learning models using FeatureExtraction technique named FAMD.\n",
    "        Parameters:\n",
    "        --------- \n",
    "        train_indx : np.ndarray : A numpy ndarray of training indeces. \\n\n",
    "        test_indx : np.ndarray :  A numpy ndarray of testing indices . \\n\n",
    "        split_indx : int : Split indicator. \\n\n",
    "        --------\n",
    "\n",
    "        Returns: \n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        X_train:np.ndarray = self.X[train_idx, :] #Training set of possible predictors.\n",
    "        X_test:np.ndarray = self.X[test_idx, :] #Testing set of possible predictors.\n",
    "\n",
    "        y_train:np.ndarray = self.y[train_idx] #Training set of target variable.\n",
    "        y_test:np.ndarray = self.y[test_idx] #Testing set of target variable\n",
    "\n",
    "\n",
    "        predictors_transformers = self.transform_predictors(num_predictors_idx = self.num_predictors_idx, PCA_predictors_idx = self.PCA_predictors_idx, \n",
    "                                                            cat_predictors_idx = self.cat_predictors_idx, train_type = \"FE\")\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "        for model_name in self.model_names:\n",
    "            model = self.Models[model_name] #The instance of the machine learning model.\n",
    "\n",
    "            \n",
    "            model_trans: Pipeline  = Pipeline(steps = [(\"Preprocessing\", predictors_transformers), #Define the pipeline for classification.\n",
    "                                                       (\"FAMD\", prince.FAMD(n_components = 2, n_iter = 3)),\n",
    "                                                       (\"Classifier\", model)])\n",
    "            \n",
    "\n",
    "            model_trans.fit(X = X_train, y = y_train)\n",
    "\n",
    "            y_pred:np.ndarray = model_trans.predict(X = X_test)\n",
    "\n",
    "            \n",
    "    \n",
    "            self.FactVsPrediction[(model_name, \"FE\", split_idx, \"True\")] = y_test #Save actual labels to the comparison dataframe.\n",
    "            self.FactVsPrediction[(model_name, \"FE\", split_idx, \"Pred\")] = y_pred #Save predicted labels to the comparison dataframe.\n",
    "\n",
    "\n",
    "                                                       \n",
    "\n",
    "    def train_models(self) -> None:\n",
    "        \"\"\"\"The  method  train the models with two versions: without FeatureSelection and with FeatureSelection. To split the dataset into training and testing subsets,\n",
    "        a StratifiedShuffleSplit is defined so that the probabilities of targer's modalities are preserved.\n",
    "\n",
    "        Parameters: \\n\n",
    "        ---------\n",
    "        \n",
    "        ---------\n",
    "\n",
    "        Returns: \\n\n",
    "        None\n",
    "        \"\"\"\n",
    "        #Define an instance of StratifiedShuffleSplit.\n",
    "        SSS_inst: StratifiedShuffleSplit = StratifiedShuffleSplit(n_splits = self.n_splits, \n",
    "                                                                  test_size = self.test_size, \n",
    "                                                                  train_size = self.train_size)\n",
    "        \n",
    "\n",
    "        self.X:np.ndarray = self.Dataset[self.predictors].to_numpy() #The set X of predictors.\n",
    "        self.y:np.ndarray = self.Dataset[self.target_var_discr].to_numpy()  #Discretized target variable.\n",
    "\n",
    "\n",
    "        for split_indx, indx_tup in enumerate(SSS_inst.split(X = self.X, y = self.y)):\n",
    "            train_indx, test_indx = indx_tup #Unpack the tuple of training index and testing index.\n",
    "            #print(split_indx)\n",
    "            self.train_without_FS( train_indx = train_indx, test_indx = test_indx, split_indx = split_indx) #Train the models without FeatureSelection.\n",
    "            self.train_with_FS( train_indx = train_indx, test_indx = test_indx, split_indx = split_indx) #Train the model with FeatureSelection\n",
    "            self.train_with_FE(train_idx = train_indx, test_idx = test_indx, split_idx = split_indx)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    def boxplot_the_model_four_versions(self, metrics_dataframe:pd.DataFrame, metrics_names:list[str]) -> None:\n",
    "        \"\"\"Metoda porównuje wartości miary dokładności dla każdego modelu indywidualnie w wersji bez strojenia i ze strojeniem.\"\"\"\n",
    "        for model_name in self.model_names:\n",
    "            for metric_name in metrics_names:\n",
    "                boxplot_figure: plt.figure = plt.figure(num  = f\"Boxplot comparison for {model_name} model and for {metric_name} metric\")\n",
    "                boxplot_axes: plt.axes = boxplot_figure.add_subplot()\n",
    "\n",
    "\n",
    "                slicer = pd.IndexSlice\n",
    "\n",
    "                metrics_dataframe_melted:pd.DataFrame = metrics_dataframe.loc[:, slicer[model_name, :, metric_name]].melt(var_name = \"train_type\", value_name = \"metric_value\", col_level = 1)\n",
    "\n",
    "                \n",
    "        \n",
    "                sns.boxplot(data = metrics_dataframe_melted, x = \"train_type\", y = \"metric_value\",ax = boxplot_axes)\n",
    "\n",
    "\n",
    "                boxplot_axes.legend(self.training_types)\n",
    "\n",
    "                boxplot_axes.grid(True)\n",
    "                boxplot_axes.spines[[\"top\", \"right\"]].set_visible(False)\n",
    "\n",
    "                boxplot_axes.set_xlabel(\"Training type\")\n",
    "                boxplot_axes.set_ylabel(f\"Variability of the {metric_name} metric values\")\n",
    "                boxplot_axes.set_title(f\"Comparison of variability of {metric_name} values for {model_name}\")\n",
    "\n",
    "\n",
    "\n",
    "    def plot_confussion_matrix(self) -> None:\n",
    "        \"\"\"The methods computes the confussion matrix which will be plotted as a heatmap\"\"\"\n",
    "        for model_name in self.Models.keys():\n",
    "            for train_type in self.training_types:\n",
    "                slicer = pd.IndexSlice\n",
    "\n",
    "                y_true:pd.Series = self.FactVsPrediction.loc[:, slicer[model_name, train_type, 0, \"True\"]] #Find the true label for the model and train type\n",
    "\n",
    "                y_pred:pd.Series = self.FactVsPrediction.loc[:, slicer[model_name, train_type, :, \"Pred\"]].mode(axis=1)[0] #Find the predicted label for the model and train type.\n",
    "\n",
    "\n",
    "           \n",
    "\n",
    "                axes = plt.figure(num = f\"{model_name}_conf_matrix_{train_type}\").add_subplot()\n",
    "\n",
    "\n",
    "                ConfusionMatrixDisplay.from_predictions(y_true = y_true, y_pred = y_pred, normalize = \"true\", ax = axes)\n",
    "                axes.set_title(f\"Confusion matrix for {model_name}, {train_type}\")\n",
    "\n",
    "\n",
    "    def plot_models_results_collectively(self, metrics_dataframe:pd.DataFrame, metrics_names:list[str]):\n",
    "        for metric_name in metrics_names:\n",
    "            for train_type in self.training_types:\n",
    "                metric_figure:plt.Figure = plt.figure(num =f\"Comparison of models with respect to {metric_name} metric and {train_type} training type\")\n",
    "                metric_axes:plt.axes = metric_figure.add_subplot()\n",
    "\n",
    "\n",
    "                #The x-cordinates.\n",
    "                x_values:list[int] = list(range(0, self.n_splits))\n",
    "\n",
    "                for model_name in self.model_names:\n",
    "                    y_values:list[float] = metrics_dataframe[(model_name, train_type, metric_name)]\n",
    "\n",
    "                    metric_axes.plot(x_values, y_values)\n",
    "\n",
    "                metric_axes.set_title(f\"Comparison of models with respect to {metric_name} metric and {train_type} training type\")\n",
    "                metric_axes.set_xlabel(\"Iteration_idx\")\n",
    "                metric_axes.set_ylabel(\"Metric value\") \n",
    "\n",
    "                metric_axes.legend(self.model_names)\n",
    "                metric_axes.set_xticks(x_values)\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "                    \n",
    "    def compute_perf_metric(self, metrics: dict[str, callable], metrics_names:list[str]) -> pd.DataFrame:\n",
    "        \"\"\"The function assess the perfomance of a given model, by a given training_type, by a given metric_name, by a given split_idx.\n",
    "\n",
    "        Parameters: \\n\n",
    "        ---------\n",
    "        metrics : dict[str, \"metric\"] : Dictionary holding the callable metrics. \\n\n",
    "        ---------\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame\n",
    "    \n",
    "        \"\"\"\n",
    "        #The column-system of a metric_dataframe will be 3-leveled. The first level is model_name, the second is training_type, the third is metric_name.\n",
    "        \n",
    "        col_indeces = pd.MultiIndex.from_product(iterables =[self.model_names, self.training_types, metrics_names ]) \n",
    "        row_indeces: list[int] = list(range(self.n_splits))\n",
    "\n",
    "        metrics_dataframe:pd.DataFrame = pd.DataFrame(data = None, index = row_indeces, columns = col_indeces)\n",
    "        \n",
    "\n",
    "        for model_name in self.model_names:\n",
    "            for train_type in self.training_types:\n",
    "                for metric_name in metrics_names:\n",
    "                    for split_idx in row_indeces:\n",
    "                        y_true:pd.Series = self.FactVsPrediction[(model_name, train_type, split_idx, \"True\")]\n",
    "                        y_pred:pd.Series = self.FactVsPrediction[(model_name, train_type, split_idx, \"Pred\")]\n",
    "\n",
    "                        metric_value:float = metrics[metric_name](y_true = y_true, y_pred = y_pred, \n",
    "                                                                  average = \"weighted\", zero_division = 0)\n",
    "                        \n",
    "                    \n",
    "                        metrics_dataframe.loc[split_idx,(model_name, train_type, metric_name)] = metric_value\n",
    "                      \n",
    "\n",
    "        \n",
    "        return metrics_dataframe\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compare_models(self) -> None:\n",
    "        \"\"\"Metoda wylicza, na podstawie przewidzianych przez modele etykiet, miary dokładności modelu, takie jak: accuracy_score, f1_score, precision_score, recall score.\n",
    "        Następnie wyniki tych metryk przedstawia na wykresach.\"\"\"\n",
    "        #Zdefiniuj różne miary dokładności modeli.\n",
    "        metrics:dict[str : \"metric\"] = {#\"Precision\":precision_score,\n",
    "                #\"Recall\":recall_score,\n",
    "                  \"F1\": f1_score}\n",
    "        \n",
    "        metrics_names:list[str] = list(metrics.keys())\n",
    "        \n",
    "\n",
    "        print(self.FactVsPrediction.isna().sum().sum())\n",
    "\n",
    "\n",
    "        metrics_dataframe:pd.DataFrame = self.compute_perf_metric(metrics  = metrics, metrics_names = metrics_names)\n",
    "\n",
    "        \n",
    "\n",
    "        #self.plot_models_results_collectively(metrics_dataframe = metrics_dataframe, metrics_names = metrics_names)\n",
    "        self.boxplot_the_model_four_versions(metrics_dataframe = metrics_dataframe, metrics_names = metrics_names)\n",
    "        #self.plot_confussion_matrix()\n",
    "\n",
    "                    \n",
    "              \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prince\\pca.py:175: PerformanceWarning: Concatenating sparse arrays with multiple fill values: '[-0.1401346422614348, -0.11342879298653222, -0.2747792564031168, -0.11637548610733041, -0.3775483358388714, -0.15935369720403486, -0.20407230417833816, -0.3967875061517063, -0.15613409871034392, -0.26216790968021064, -0.14253027976565605, -0.408973329226932, -0.30179195082237514, -0.2420218431072968, -0.23778789808576334, -0.17063995897339673, -0.20817879828045857, -0.2543010749801814, -0.252966151980339, -0.15395029747212424, -0.21300211294375337, -0.1582877742228606, -0.1261480208956645, -0.2145857935811207, -0.34127991794679347, -0.23991421072656197, -0.18944556601251256, -0.3255394918048563, -0.16354793239068163, -0.11782119975151073, -0.19818031164442299, -0.2978394066808476, -0.23707488929002463, -0.18122469584548853, -0.5236896874829868, -0.41838565335683187, -0.5507893284863741, -0.30291177637970884, -0.15061523846959088, -0.1938621396834383, -0.3788911320250251, -0.14720463996337863, -0.2685476239101185, -0.40439434866614205, -0.5701211190626242, -0.4465696045578052, -0.3558502813246158, -0.09014406392009665, -0.12479880540984786, -0.4581707469755359, -0.119249387650643, -0.3680115999084465, -0.30123047693918176, -0.1867454911083463, -0.354420200731144, -0.1301117489252866, -0.2065459981823903, -0.084322050660113, -0.5953945476268904, -0.29440927992675725, -0.5736733099345255, -0.14371312394574168, -0.2850605595313121, -0.18492354031047958, -0.09561221832242091, -0.22760260468126076, -0.49749669605612606, -0.07360231998168931, -0.22157232431951146, -0.3144292487938064, -0.9910723885928734, -0.9326389832981615]'. Picking the first and converting the rest.\n",
      "  X = self.scaler_.transform(X.to_numpy())\n",
      "c:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prince\\pca.py:175: PerformanceWarning: Concatenating sparse arrays with multiple fill values: '[-0.1401346422614348, -0.11342879298653222, -0.2747792564031168, -0.11637548610733041, -0.3775483358388714, -0.15935369720403486, -0.20407230417833816, -0.3967875061517063, -0.15613409871034392, -0.26216790968021064, -0.14253027976565605, -0.408973329226932, -0.30179195082237514, -0.2420218431072968, -0.23778789808576334, -0.17063995897339673, -0.20817879828045857, -0.2543010749801814, -0.252966151980339, -0.15395029747212424, -0.21300211294375337, -0.1582877742228606, -0.1261480208956645, -0.2145857935811207, -0.34127991794679347, -0.23991421072656197, -0.18944556601251256, -0.3255394918048563, -0.16354793239068163, -0.11782119975151073, -0.19818031164442299, -0.2978394066808476, -0.23707488929002463, -0.18122469584548853, -0.5236896874829868, -0.41838565335683187, -0.5507893284863741, -0.30291177637970884, -0.15061523846959088, -0.1938621396834383, -0.3788911320250251, -0.14720463996337863, -0.2685476239101185, -0.40439434866614205, -0.5701211190626242, -0.4465696045578052, -0.3558502813246158, -0.09014406392009665, -0.12479880540984786, -0.4581707469755359, -0.119249387650643, -0.3680115999084465, -0.30123047693918176, -0.1867454911083463, -0.354420200731144, -0.1301117489252866, -0.2065459981823903, -0.084322050660113, -0.5953945476268904, -0.29440927992675725, -0.5736733099345255, -0.14371312394574168, -0.2850605595313121, -0.18492354031047958, -0.09561221832242091, -0.22760260468126076, -0.49749669605612606, -0.07360231998168931, -0.22157232431951146, -0.3144292487938064, -0.9910723885928734, -0.9326389832981615]'. Picking the first and converting the rest.\n",
      "  X = self.scaler_.transform(X.to_numpy())\n",
      "c:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prince\\pca.py:175: PerformanceWarning: Concatenating sparse arrays with multiple fill values: '[-0.13768567816430285, -0.11039413601923785, -0.2920754301067413, -0.09013643461417804, -0.37885906472929465, -0.19471695340449421, -0.21770017209205406, -0.4064476955043193, -0.1471921813589838, -0.24130092012986182, -0.13267723930930272, -0.4259681711943791, -0.2826511494912659, -0.2356224560050802, -0.1725981317922415, -0.20816117915315516, -0.285036433547643, -0.23847859005514926, -0.1425182167738215, -0.24409061882084682, -0.3186804207383157, -0.21456763220039493, -0.1686298281958028, -0.2753713563286057, -0.1802728692283561, -0.15612088436486637, -0.30344445550408056, -0.20488284546444085, -0.19120825244298942, -0.5357876275681547, -0.5653022989825318, -0.27290160817670156, -0.2015511950467671, -0.3928956096142636, -0.22980371949168696, -0.4081100652049614, -0.589917543553573, -0.44463247357856783, -0.3412510338607575, -0.09735847670224711, -0.16039896095578043, -0.1274721682953263, -0.47837458941669736, -0.12204530941042341, -0.3490969101472253, -0.31006584139283366, -0.19816353873493972, -0.31654875520387676, -0.08228293081879129, -0.6113350430557499, -0.5688839755674383, -0.15172222775204028, -0.2327312734314835, -0.47978781137125587, -0.0735960906794919, -0.3249915367889261, -0.998305944912648, -0.9258200997725514]'. Picking the first and converting the rest.\n",
      "  X = self.scaler_.transform(X.to_numpy())\n",
      "c:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prince\\pca.py:175: PerformanceWarning: Concatenating sparse arrays with multiple fill values: '[-0.13892133247839358, -0.11342879298653222, -0.2729247070474676, -0.11782119975151073, -0.37258334786843855, -0.166624338623656, -0.21060426416860525, -0.3955054753168237, -0.15504604295474128, -0.2543010749801814, -0.4191941288033016, -0.2967004372825619, -0.24272032029729387, -0.23920753994049024, -0.1716291844139873, -0.2145857935811207, -0.260873245423434, -0.24549435688400797, -0.16041253740516406, -0.21849478146555132, -0.13268846934354428, -0.21537326702960904, -0.33476921142719646, -0.2334771860611605, -0.16763726186602643, -0.26537693868708856, -0.18764983291616696, -0.32605910824378453, -0.11637548610733041, -0.19988146140104263, -0.29498373760456637, -0.2261101124880495, -0.18215644849540122, -0.5233663224089419, -0.4102132714228836, -0.5575102455974827, -0.3023523820401334, -0.14720463996337863, -0.19646443249388615, -0.20156825469342232, -0.3828912665819324, -0.1401346422614348, -0.2634562118349593, -0.4035562247627676, -0.5760292677326193, -0.44846105565116157, -0.35106069172883597, -0.09561221832242091, -0.13140642508098588, -0.14948701855038718, -0.4600144998855582, -0.119249387650643, -0.3605762556803866, -0.30010437775659166, -0.34912645832199124, -0.21693963870013047, -0.084322050660113, -0.5976648877901168, -0.29440927992675725, -0.572787327459966, -0.14605007594171251, -0.2862458476356305, -0.18492354031047958, -0.17456322916099562, -0.10729289679056035, -0.23202248538170148, -0.48856918408593386, -0.07126513988282802, -0.21220584057545525, -0.3187073944080697, -0.9917554158197751, -0.9326389832981615]'. Picking the first and converting the rest.\n",
      "  X = self.scaler_.transform(X.to_numpy())\n",
      "c:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prince\\pca.py:175: PerformanceWarning: Concatenating sparse arrays with multiple fill values: '[-0.13892133247839358, -0.11342879298653222, -0.2729247070474676, -0.11782119975151073, -0.37258334786843855, -0.166624338623656, -0.21060426416860525, -0.3955054753168237, -0.15504604295474128, -0.2543010749801814, -0.4191941288033016, -0.2967004372825619, -0.24272032029729387, -0.23920753994049024, -0.1716291844139873, -0.2145857935811207, -0.260873245423434, -0.24549435688400797, -0.16041253740516406, -0.21849478146555132, -0.13268846934354428, -0.21537326702960904, -0.33476921142719646, -0.2334771860611605, -0.16763726186602643, -0.26537693868708856, -0.18764983291616696, -0.32605910824378453, -0.11637548610733041, -0.19988146140104263, -0.29498373760456637, -0.2261101124880495, -0.18215644849540122, -0.5233663224089419, -0.4102132714228836, -0.5575102455974827, -0.3023523820401334, -0.14720463996337863, -0.19646443249388615, -0.20156825469342232, -0.3828912665819324, -0.1401346422614348, -0.2634562118349593, -0.4035562247627676, -0.5760292677326193, -0.44846105565116157, -0.35106069172883597, -0.09561221832242091, -0.13140642508098588, -0.14948701855038718, -0.4600144998855582, -0.119249387650643, -0.3605762556803866, -0.30010437775659166, -0.34912645832199124, -0.21693963870013047, -0.084322050660113, -0.5976648877901168, -0.29440927992675725, -0.572787327459966, -0.14605007594171251, -0.2862458476356305, -0.18492354031047958, -0.17456322916099562, -0.10729289679056035, -0.23202248538170148, -0.48856918408593386, -0.07126513988282802, -0.21220584057545525, -0.3187073944080697, -0.9917554158197751, -0.9326389832981615]'. Picking the first and converting the rest.\n",
      "  X = self.scaler_.transform(X.to_numpy())\n",
      "c:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prince\\pca.py:175: PerformanceWarning: Concatenating sparse arrays with multiple fill values: '[-0.1425182167738215, -0.11039413601923785, -0.2989487335556314, -0.08228293081879129, -0.39803171792790815, -0.1686298281958028, -0.19120825244298942, -0.41141465409395644, -0.15172222775204028, -0.27290160817670156, -0.1471921813589838, -0.38418287227573517, -0.30344445550408056, -0.2327312734314835, -0.2356224560050802, -0.1802728692283561, -0.26020147394144394, -0.26789381378407734, -0.22383377138082167, -0.16456586163758258, -0.24130092012986182, -0.345196263584483, -0.1274721682953263, -0.26279060710407043, -0.18763395124943666, -0.28974809875340324, -0.11636563671574175, -0.1725981317922415, -0.3144026372031537, -0.249576486228162, -0.537049790983981, -0.43849931812637044, -0.5383089950360886, -0.2753713563286057, -0.22980371949168696, -0.37706775887992583, -0.16039896095578043, -0.2522746888191955, -0.5664987073028162, -0.4369525784984748, -0.3605457384567122, -0.0735960906794919, -0.13768567816430285, -0.15612088436486637, -0.4712449120101604, -0.12204530941042341, -0.37885906472929465, -0.19471695340449421, -0.3392612149512671, -0.19816353873493972, -0.6024099648423552, -0.2920754301067413, -0.5724432426711766, -0.21456763220039493, -0.5151726347564433, -0.23847859005514926, -0.30787453590353964, -0.9955894611156906, -0.9258200997725514]'. Picking the first and converting the rest.\n",
      "  X = self.scaler_.transform(X.to_numpy())\n",
      "c:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prince\\pca.py:175: PerformanceWarning: Concatenating sparse arrays with multiple fill values: '[-0.13892133247839358, -0.119249387650643, -0.28683665496726274, -0.10729289679056035, -0.38597382221323245, -0.1696449653077703, -0.20072662991238713, -0.3942192752429576, -0.15061523846959088, -0.2602234978505732, -0.14371312394574168, -0.4073142013756565, -0.29612930982848606, -0.2406188061077461, -0.23920753994049024, -0.17359072449571242, -0.21060426416860525, -0.2576081199359126, -0.24549435688400797, -0.15613409871034392, -0.22157232431951146, -0.16354793239068163, -0.12343484308556128, -0.22233505402549258, -0.333248677247312, -0.23707488929002463, -0.1614644341671908, -0.2647382447678514, -0.18308345930920603, -0.320296967333262, -0.15504604295474128, -0.10885929930822526, -0.19473343458895, -0.30291177637970884, -0.2298231704815641, -0.1885498372781937, -0.5249811559977179, -0.4171700023998841, -0.5520173998626698, -0.29953974060966126, -0.15173506978750717, -0.19210769509393696, -0.20817879828045857, -0.37167349706498853, -0.14605007594171251, -0.26664973706206163, -0.4077296162734033, -0.5721959103405009, -0.4495921067681882, -0.3553742272234162, -0.09382491645808348, -0.13395824439317136, -0.4600144998855582, -0.36244939169097706, -0.2967004372825619, -0.19560084839022762, -0.3505781338488767, -0.1261480208956645, -0.20324104897507353, -0.09014406392009665, -0.5993619992643682, -0.29555707874339193, -0.577203640547467, -0.1401346422614348, -0.28801463386630966, -0.17456322916099562, -0.17840024201717333, -0.10408939914022929, -0.23420114806551587, -0.48822255815788274, -0.1572146243969032, -0.07360231998168931, -0.3144292487938064, -0.988506833192702, -0.9378882300937029]'. Picking the first and converting the rest.\n",
      "  X = self.scaler_.transform(X.to_numpy())\n",
      "c:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prince\\pca.py:175: PerformanceWarning: Concatenating sparse arrays with multiple fill values: '[-0.13892133247839358, -0.119249387650643, -0.28683665496726274, -0.10729289679056035, -0.38597382221323245, -0.1696449653077703, -0.20072662991238713, -0.3942192752429576, -0.15061523846959088, -0.2602234978505732, -0.14371312394574168, -0.4073142013756565, -0.29612930982848606, -0.2406188061077461, -0.23920753994049024, -0.17359072449571242, -0.21060426416860525, -0.2576081199359126, -0.24549435688400797, -0.15613409871034392, -0.22157232431951146, -0.16354793239068163, -0.12343484308556128, -0.22233505402549258, -0.333248677247312, -0.23707488929002463, -0.1614644341671908, -0.2647382447678514, -0.18308345930920603, -0.320296967333262, -0.15504604295474128, -0.10885929930822526, -0.19473343458895, -0.30291177637970884, -0.2298231704815641, -0.1885498372781937, -0.5249811559977179, -0.4171700023998841, -0.5520173998626698, -0.29953974060966126, -0.15173506978750717, -0.19210769509393696, -0.20817879828045857, -0.37167349706498853, -0.14605007594171251, -0.26664973706206163, -0.4077296162734033, -0.5721959103405009, -0.4495921067681882, -0.3553742272234162, -0.09382491645808348, -0.13395824439317136, -0.4600144998855582, -0.36244939169097706, -0.2967004372825619, -0.19560084839022762, -0.3505781338488767, -0.1261480208956645, -0.20324104897507353, -0.09014406392009665, -0.5993619992643682, -0.29555707874339193, -0.577203640547467, -0.1401346422614348, -0.28801463386630966, -0.17456322916099562, -0.17840024201717333, -0.10408939914022929, -0.23420114806551587, -0.48822255815788274, -0.1572146243969032, -0.07360231998168931, -0.3144292487938064, -0.988506833192702, -0.9378882300937029]'. Picking the first and converting the rest.\n",
      "  X = self.scaler_.transform(X.to_numpy())\n",
      "c:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prince\\pca.py:175: PerformanceWarning: Concatenating sparse arrays with multiple fill values: '[-0.1425182167738215, -0.08228293081879129, -0.24130092012986182, -0.1274721682953263, -0.34322931729655887, -0.15612088436486637, -0.22980371949168696, -0.4163223583063103, -0.1686298281958028, -0.249576486228162, -0.43227915452169074, -0.30566752152787496, -0.2356224560050802, -0.16039896095578043, -0.19816353873493972, -0.27290160817670156, -0.26789381378407734, -0.13267723930930272, -0.21138867672431735, -0.15172222775204028, -0.1471921813589838, -0.21456763220039493, -0.35103097985101805, -0.2268383859742172, -0.26535447861860545, -0.20488284546444085, -0.3144026372031537, -0.18763395124943666, -0.19471695340449421, -0.2826511494912659, -0.5307089572372109, -0.41141465409395644, -0.5604911280935958, -0.2874019216887492, -0.20816117915315516, -0.41956226988062195, -0.13768567816430285, -0.23847859005514926, -0.39461507345131897, -0.5818281835787088, -0.4712449120101604, -0.12204530941042341, -0.37164204063083583, -0.3270681811568559, -0.33322047288204226, -0.05204029478828879, -0.5956283983293708, -0.5544184556519577, -0.26020147394144394, -0.17647722583664432, -0.1802728692283561, -0.09735847670224711, -0.5164851786283143, -0.0735960906794919, -0.24409061882084682, -0.3249915367889261, -1.0084275887540588, -0.9043639030052577]'. Picking the first and converting the rest.\n",
      "  X = self.scaler_.transform(X.to_numpy())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The datatype of each feature of the dataframe.\n",
    "dtypes = { \"Make\": \"string\", \n",
    "            \"Model\":\"string\",\n",
    "            \"Vehicle Class\":\"string\",\n",
    "            \"Engine Size(L)\":np.float64,\n",
    "            \"Cylinders\":np.int16,\n",
    "            \"Transmission\":\"string\",\n",
    "            \"Fuel Type\":\"string\",\n",
    "            \"Fuel Consumption City (L/100 km)\":np.float64,\n",
    "            \"Fuel Consumption Hwy (L/100 km)\":np.float64,\n",
    "            \"Fuel Consumption Comb (L/100 km)\":np.float64,\n",
    "            \"Fuel Consumption Comb (mpg)\":np.float64,\n",
    "            \"CO2 Emissions(g/km)\":np.float64}\n",
    "\n",
    "\n",
    "# Define the dictionary of  ML models we will be training.\n",
    "Models = {\n",
    "    \"DrzewkoDecyzyjne\": DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\", min_samples_split=2), \n",
    "   # \"LasLosowy\": RandomForestClassifier(n_estimators=15, criterion='gini'), \n",
    "   # \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "   # \"RegresjaLogistyczna\": LogisticRegression(multi_class = \"ovr\", max_iter = 250      )\n",
    " #  \"RegresjaLiniowa\":MultiOutputLinearRegression(LinearRegression())\n",
    "}    \n",
    "\n",
    "#For each ML model create it's corresponding parameter space.\n",
    "Models_hipparams = {\"DrzewkoDecyzyjne\":{\"criterion\":['gini','entropy'],\n",
    "                                       \"splitter\":['best','random'],\n",
    "                                       \"min_samples_split\":[2,3],\n",
    "                                        \"min_samples_leaf\":[2,3]},\n",
    "\n",
    "                    \"LasLosowy\":{\"n_estimators\":list(range(5, 25, 5)),\n",
    "                                        \"min_samples_split\":[2,3],\n",
    "                                       \"min_samples_leaf\":[2,3]},\n",
    "                                       \n",
    "                  \"KNN\": {\"n_neighbors\":list(range(1, 10, 2)),\n",
    "                            \"p\":[1,2]},\n",
    "                            \n",
    "                    #    \"RegresjaLogistyczna\": {\n",
    "                      #  \"max_iter\":[250, 300]\n",
    "                     #       \"penalty\":['l2'],\n",
    "                      #        \"solver\":['liblinear','newton-cg',],\n",
    "                              \n",
    "                     #   },\n",
    "                     \"RegresjaLiniowa\":{},\n",
    "                             }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "target_var: str = \"CO2 Emissions(g/km)\" #The target variable's name\n",
    "file_name:str = \"CO2Emission.csv\" #The filename\n",
    "\n",
    "\n",
    "n_splits:int = 3 # How many times we'll be learning each model.\n",
    "train_size:float = 0.8 #The size of training set\n",
    "test_size:float  = 1 - train_size #The size of testing set.\n",
    "\n",
    "\n",
    "WielkiEstymator = ModelsComparisom(Filename = file_name, target_var = target_var, dtypes = dtypes, \n",
    "                                        Models = Models, Models_hipparams = Models_hipparams, \n",
    "                                        n_splits = n_splits, train_size  = train_size, test_size = test_size, \n",
    "                                        bins = [-float('inf'), 150, 250, float('inf')], show_plots = False\n",
    "                                        )\n",
    "\n",
    "WielkiEstymator.descriptive_statistics()\n",
    "WielkiEstymator.train_models()\n",
    "WielkiEstymator.compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pytania badawcze przykładowe:\n",
    "# 1) Jak liczba klas docelowych wpływa na skuteczność metod? Czy skuteczność modelu domyślnie maleje, jeżeli liczba klas docelowych wzrośnie?\n",
    "# 2) Jak skuteczne są metody proste w porównaniu z metodami bardziej zaawansowanymi.\n",
    "# 3) Jak istotne jest strojenie parametrów? Czy statyczne strojenie parametrów ulega dynamicznego strojeniu parametrów.\n",
    "# 4) Jak istotny jest wybór optymalnych cech? Czy należy uwzględnić wszystkie względne? A może wystarczy tylko kilka cech?\n",
    "# 5) Jak prezentuje się dokładnośc predykcji w stosunku do poszczególnych klas? Czy klasy rzadsze są łatwiej przewidywalne?\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "#Typy wyboru cech.\n",
    "#1) Filtry, czyli  kasujemy cechy, niezależnie od modelu, na podstawie statystyk.\n",
    "#2) wrappers, czyli metody, które są \"owinięte\" wokół pewnego modelu.\n",
    "#3) embedded methods.\n",
    "\n",
    "\n",
    "#Jak sobie poradzić ze zmiennymi kategorycznymi wysoko-kardynalnymi \n",
    "#1)Weź to przeczytaj: https://github.com/rasbt/mlxtend/issues/502\n",
    "\n",
    "# Najpierw ustal klasy, które są determinowane przez regulacje. Będzie ich trzy\n",
    "# Potem ustaw własne progi dla klas emisyjności, aby liczba klas wzrosła do 5.\n",
    "# Beamer w latexu do robienia prezentacji.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co dalej do roboty?:\n",
    "2) Zamknięćie wszystkich funkcji w jedną, potężna funkcję.\n",
    "4) Liczenie wskaźników dokładności, jeżeli selekcja cech była wybierana automatycznie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
