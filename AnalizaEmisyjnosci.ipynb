{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sekcja odczytywania danych, określania typów danych cech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, r2_score\n",
    "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "import timeit\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregator rzadkich klas danej zmienej kategorycznej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RareClassAggregator(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self,  q:float = 0.1) -> None:\n",
    "        assert isinstance(q, float) and 0< q <1, \"Argument 'q' musi być liczbą zmiennoprzecinkową z przedziału (0;1)\"\n",
    "    \n",
    "        self.q: float = q\n",
    "\n",
    "\n",
    "    def validate_X(self, X:pd.DataFrame, cat_features:list[str]) -> None:\n",
    "        \"Sprawdza czy x spełnia warunki\"\n",
    "        assert isinstance(X, pd.DataFrame), \"Argument 'X' musi być instancją klasy pd.DataFrame!\"\n",
    "\n",
    "        assert isinstance(self.cat_features, list)\n",
    "\n",
    "        assert set(cat_features).issubset(self.cat_features), \"Zbyt wiele zmiennych kategorycznych przekazałeś!\"\n",
    "        assert len(cat_features) > 0, \"Przekazałeś pusty zbiór cech kategorycznych!\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X:pd.DataFrame, y: None = None, cat_features:list[str] = [] ) -> \"RareClassAggregator\":\n",
    "        \"\"\"Wylicz próg częstowliwościowy dla zmiennej kategorycznej 'cat_feature'.\n",
    "        Opis argumentów:\n",
    "        ---------\n",
    "        X:pd.DataFrame - Ramka danych typu pandas, która zawiera ceche 'cat_feature', której rzadkie cechy chcemy połączyć w jedną klase.\n",
    "        y:pd.DataFrame - Argument nieuzywany, służy do zachowania spójności transformatotów.\n",
    "        cat_features:list[str] - Lista nazw zmiennych kategorycznych.\n",
    "        \"\"\"\n",
    "\n",
    "        #'y' musi być stale równy None. Nie jest on potrzebny dla tego estimatora.\n",
    "        if y is not None:\n",
    "            raise ValueError(\"Argument 'y' musi być zawsze ustawiony na wartość None!\")\n",
    "        \n",
    "        self.X_train: pd.DataFrame = X #Zdefiniuj zbiór treningowy.\n",
    "        self.cat_features = cat_features\n",
    "        \n",
    "        self.validate_X(X = X, cat_features = self.cat_features) #Sprawx, czy argument X spełnia podstawowe założenia.\n",
    "\n",
    "\n",
    "      \n",
    "        self.freq_thresholds:dict[str, float] = {} #Tabela przechowująca wszystkie progi częstotliwościowe dla każdej cechy kategorycznej\n",
    "        self.cross_tabs: list[pd.Series] = {} #Słownik służący do przechowywania tabeli krzyżowych każdej zmiennej kategorycznej.\n",
    "\n",
    "        for cat_feature in self.cat_features:\n",
    "            class_crosstab:pd.Series = ( self.X_train[cat_feature]. #Znajdź tabelę krzyżową dla zmiennej kategorycznej 'cat_feature'\n",
    "                                         value_counts(normalize = True, sort = True))\n",
    "\n",
    "            self.freq_thresholds[cat_feature] = class_crosstab.quantile(q = self.q) #Wylicz próg częstotliwościowy dla zmiennej 'cat_feature'\n",
    "\n",
    "            self.cross_tabs[cat_feature] = class_crosstab\n",
    "\n",
    "        \n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X:pd.DataFrame, cat_features:list[str]) -> pd.DataFrame:\n",
    "        \"\"\"Transformuje ramkę danych X, agregując rzadkie klasy zmiennej kategorycznej 'cat_feature' \"\"\"\n",
    "        self.validate_X(X = X, cat_features = cat_features) #Upewnij się, że X jest ramką danych oraz, że cat_features jest NIEPUSTYM podzbiorem zbioru cech X.\n",
    "\n",
    "\n",
    "        for cat_feature in cat_features:\n",
    "            class_freqtable:pd.Series =  X[cat_feature].value_counts(normalize = True, sort = False) #Wyestymuj znormalizowaną tabelę krzyżową.\n",
    "\n",
    "\n",
    "            #Stwórz agregowaną kolumnę cechy kategorycznej.\n",
    "            aggregated_col:pd.Series =  X[cat_feature].apply(func = lambda v:   v if class_freqtable[v] >= self.freq_thresholds[cat_feature] else \"Other\").astype(dtype = \"string\")\n",
    "                                        \n",
    "            \n",
    "\n",
    "            X.loc[:, cat_feature]  = aggregated_col\n",
    "     \n",
    "        return X\n",
    "    \n",
    "\n",
    "   \n",
    "\n",
    "    def get_feature_names_out(self,) -> None:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podrasowany SequentialFeatureSelector z wewnętrznym mechanizmem OHE dla zmiennych kategorycznych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedSequentialFeatureSelection(SFS):\n",
    "    def __init__(self,  cat_vars:list[str],**kwargs) -> None:\n",
    "        \"\"\"\"\n",
    "        Opis argumentów: \\n\n",
    "        ---------\n",
    "        cat_vars:list[str] - Lista ciągów znaków, które identyfikują zmienne kategoryczne \\n\n",
    "\n",
    "        \"\"\"\"\"\"\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.cat_vars:np.ndarray[str] = cat_vars #Indeksy wszystkich zmiennych kategorycznych przyszłej ramki danych X.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def convert_to_numpy(self, X:pd.DataFrame):\n",
    "        #The method converts X to numpy.array after categorical variables of X are Labeled.\n",
    "        for cat_var in self.cat_vars:\n",
    "            X[cat_var] = LabelEncoder().fit_transform(y = X[cat_var])\n",
    "    \n",
    "\n",
    "        return np.array(X)\n",
    "\n",
    "        \n",
    "    def fit(self, X:pd.DataFrame, y: np.ndarray):\n",
    "        #Jeżeli X jest ramką danych, przekształć X do tablicy numpy, zachowując indeksy zmiennych kategorycznych.\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            features = X.columns.to_list()\n",
    "\n",
    "            self.cat_vars_idx:list[int] = [features.index(cat_feat) for cat_feat in self.cat_vars]\n",
    "            \n",
    "            X = self.convert_to_numpy(X = X.copy())\n",
    "    \n",
    "        super().fit(X = X, y = y)\n",
    "\n",
    "    def return_cat_var_indx(self) -> list[int]:\n",
    "        return self.cat_vars_idx\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def _get_best_new_feature_score(self, estimator, X:np.ndarray, y:np.ndarray, cv:int, current_mask):\n",
    "        # Return the best new feature and its score to add to the current_mask,\n",
    "        # i.e. return the best new feature and its score to add (resp. remove)\n",
    "        # when doing forward selection (resp. backward selection).\n",
    "        # Feature will be added if the current score and past score are greater\n",
    "        # than tol when n_feature is auto,\n",
    "  \n",
    "        candidate_feature_indices = np.flatnonzero(~current_mask) #Indeksy zmiennych, które nie zostały jeszcze wybrane.\n",
    "        scores = {}\n",
    "    \n",
    "        for feature_idx in candidate_feature_indices:\n",
    "            candidate_mask = current_mask.copy() #Tworzymy kopię tablicy maskowej cech, które już wybraliśmy.\n",
    "            candidate_mask[feature_idx] = True #W miejsce feature_indx wstawiamy True. Udajemy, że wybraliśmy cechę feature_indx.\n",
    "\n",
    "            candidate_cat_vars:list[int] = [i for i in self.cat_vars_idx if candidate_mask[i] == True] #Indeksy zmiennych kategorycznych kandydatów.\n",
    "            candidate_no_cat_vars:list[int] =[i for i in np.flatnonzero(candidate_mask) if i not in self.cat_vars_idx ]  #Indeksy zmiennych niekategorycznych kandydatów.\n",
    "\n",
    "\n",
    "\n",
    "            #Kodowator OHE, który przekształca na gorąco zmienne kategoryczne, a pozostałe zmienne \"transformuje\" w sposób identycznościowy.\n",
    "            var_transformer = ColumnTransformer(transformers = [(\"OHE\", OneHotEncoder(sparse_output = False), candidate_cat_vars),\n",
    "                                                    (\"Identity\", FunctionTransformer(), candidate_no_cat_vars)], \n",
    "                                    remainder = \"drop\")\n",
    "        \n",
    "            X_new = var_transformer.fit_transform(X = X)\n",
    "\n",
    "        \n",
    "            scores[feature_idx] = cross_val_score(\n",
    "                estimator,\n",
    "                X_new,\n",
    "                y,\n",
    "                cv=cv,\n",
    "                scoring=self.scoring,\n",
    "                n_jobs=self.n_jobs,\n",
    "            ).mean()\n",
    "\n",
    "\n",
    "        new_feature_idx = max(scores, key=lambda feature_idx: scores[feature_idx])\n",
    "\n",
    "        \n",
    "        return new_feature_idx, scores[new_feature_idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie metody porównaczej.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "class ModelsComparisom():\n",
    "    def __init__(self,  Filename:str, target_var:str, dtypes:dict[str, \"datatype\"], Models:dict[str, \"estimator\"], Models_hipparams:dict[str, dict],\n",
    "                 n_splits:int = 5, train_size:float = 0.8, test_size:float = 0.2, bins:list[int] = [150, 250], show_plots:bool = True) -> None:\n",
    "        \"\"\"Konstruktor klasy, która trenuje modele. \n",
    "        Opis argumentów konstruktora:\n",
    "        ---------\n",
    "        Filename: str - Nazwa pliku, w której zawarta jest ramka danych typu pandas, która przechowuje zmienne objaśniające i zmienną objaśnianą. \\n\n",
    "        target_var:str - Nazwa zmiennej objaśnianej. \\n\n",
    "\n",
    "        dtypes:dict[str, \"datatype\"] - Typ danych każdej kolumny w pliku. \\n\n",
    "\n",
    "        Models:dict[str, \"Estimator\"] - Słownik, którego wartościami są instancje modeli, które chcemy wytrenować. Kluczami są umowne nazwy modeli. \\n\n",
    "        Models_hipparams:dict[str, dict] - Słownik, którego kluczami są umowne nazwy modeli, a którego wartościami są siatki parametrów danego modelu. \\n\n",
    "\n",
    "        n_splits:int - Liczba podziałów na zbiór treningowy i testowy. \\n\n",
    "        train_size:float - Odsetek obserwacji treningowych. \\n\n",
    "        test_size:float - Odsetek obserwacji testowych. \\n\n",
    "\n",
    "        bins:list[int]  - Wartości brzegowe dla dyskretyzacji zmiennej docelowej.\n",
    "        show_plots: bool - Czy chcesz, aby pokazywano wykresy.\n",
    "\n",
    "        ---------\n",
    "        \"\"\"\n",
    "     \n",
    "        self.dtypes:dict[str, \"datatype\"] = dtypes\n",
    "\n",
    "        self.Dataset:pd.DataFrame = self.ReadDataFrame(filename = Filename, sep = ';', dec_sep = ',') \n",
    "        self.features:list[str] = self.Dataset.columns\n",
    " \n",
    "        \n",
    "        self.Cat_features:list[str] = [feature for feature in self.features if dtypes[feature] == \"string\"]\n",
    "        self.Num_features:list[str] =  list(self.Dataset.select_dtypes(include = \"number\").columns)\n",
    "    \n",
    "     \n",
    "        self.target_var:str = target_var\n",
    "        self.bins:list[int] = bins\n",
    "\n",
    "        self.Cat_predictors:list[str] = []\n",
    "        self.Num_predictors:list[str] = []\n",
    "        self.PCA_predictors:list[str] = []\n",
    "\n",
    "        self.Models:dict[str, \"estimator\"] = Models\n",
    "        self.Models_hipparams:dict[str, dict] = Models_hipparams\n",
    "\n",
    "        self.n_splits:int = n_splits\n",
    "        self.train_size:float = train_size\n",
    "        self.test_size:float = test_size\n",
    "\n",
    "        self.show_plots:bool = show_plots\n",
    "\n",
    "\n",
    "        self.PrzygotujRamkiPorownawcze()\n",
    "        self.PrzygotujRamkiCzasowe()\n",
    "        \n",
    "\n",
    "    def ReadDataFrame(self, filename:str, \n",
    "                      sep:str =';', dec_sep:str =',') -> pd.DataFrame:\n",
    "        \"\"\"Odczytuje plik o nazwie filename i wprowadza dane z pliku do ramki danych.\"\"\"\n",
    "     \n",
    "    \n",
    "        Dataset:pd.DataFrame = pd.read_csv(filename,\n",
    "                            sep=sep, dtype = self.dtypes, decimal = dec_sep) #Wczytaj plik z danymi.\n",
    "\n",
    "\n",
    "        return Dataset\n",
    "    \n",
    "\n",
    "\n",
    "    def PlotBarPlot(self, FreqTable:pd.Series, cat_feature:str, Showxlabels:bool = False) -> None:\n",
    "        \"\"\"Funkcja rysuje histogram na bazie tabelki histogramowej. \n",
    "        1) FreqTable  - Tabela częstotliwości kategori danej zmiennej kategorycznej\n",
    "        2) CatFeature  - Cecha kategoryczna, której histogram chcemy narysować.\n",
    "        3) Showxlabel - Zmienna typu bool. Jeżeli ustawiona na True, to etykietki osi Ox są wyświetlane.\"\"\"\n",
    "        \n",
    "        plt.figure(figsize = (10,5)) #Stwórz płótno, na którym  będzie rysowany wykres\n",
    "    \n",
    "        axes = sns.barplot(x = FreqTable.index, y = FreqTable.values)\n",
    "\n",
    "\n",
    "        axes.set_ylabel(f\"Częstość klasy\") #Ustaw etykietke pionowej osi.\n",
    "\n",
    "        axes.set_xticklabels([]) #Usuń etykiety tyknięć na osi Ox.\n",
    "        axes.spines[[\"top\", \"right\"]].set_visible(False)\n",
    "\n",
    "        axes.set_title(f\"Histogram klas cechy {cat_feature}\") #Ustaw tytuł wykresu.\n",
    "\n",
    "        axes.set_ylim(0, 1.05*np.max(FreqTable))\n",
    "\n",
    "        if Showxlabels == True:\n",
    "            axes.set_xticklabels(labels = FreqTable.index)\n",
    "            \n",
    "\n",
    "\n",
    "    def DeleteFutileColsAndObs(self) -> pd.DataFrame:\n",
    "        \"\"\"Funkcja usuwa quasi-identyfikator zmienną oraz jedną obserwację, która zawiera klasę, która występuje tylko raz.\"\"\"\n",
    "\n",
    "        self.Dataset.drop(columns = [\"Model\"], inplace = True)\n",
    "\n",
    "        self.Dataset = self.Dataset.query('`Fuel Type` != \"N\"')\n",
    "\n",
    "   \n",
    "    def ComputeAndDrawCorrelationMatrix(self,) -> None:\n",
    "        \"\"\"Funkcja wylicza macierz korelaji dla zmiennych z listy FloatFeatures. Ponadto, rysuje tę macierz korelacji na wykresie, aby można\n",
    "        było sobie uzmysłowić relacje między zmiennymi\"\"\"\n",
    "\n",
    "        CorrMatrix:pd.DataFrame =  self.Dataset[self.Num_features].corr(method = \"pearson\")\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        sns.heatmap(CorrMatrix, annot=True, cmap='magma', vmin=-1, vmax=1)\n",
    "        plt.title('Macierz korelacji dla zmiennych ciągłych')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def DeleteFutileColsAndObs(self) -> None:\n",
    "        \"\"\"Funkcja usuwa quasi-identyfikator zmienną oraz jedną obserwację, która zawiera klasę, która występuje tylko raz.\"\"\"\n",
    "\n",
    "        self.Dataset.drop(columns = [\"Model\"], inplace = True)\n",
    "\n",
    "        self.Dataset = self.Dataset.query('`Fuel Type` != \"N\"')\n",
    "\n",
    "\n",
    "    def NarysujGęstości(self, Condition:str | None = None) -> None:\n",
    "        \"\"\"Ta funkcja rysuje wykresy gęstości prawdopodobieństwa dla zmiennych ciągłych\"\"\"\n",
    "\n",
    "        for float_feature in self.Num_features:\n",
    "            figure = plt.figure(num = f\"KDE_plot_{float_feature}\")\n",
    "            axes = figure.add_subplot()\n",
    "\n",
    "            sns.kdeplot(data = self.Dataset, x = float_feature, ax = axes, hue = Condition)\n",
    "            axes.set_title(f\"Wykres gęstości prawdopodobieństwa dla zmiennej {float_feature}\")\n",
    "\n",
    "\n",
    "\n",
    "    def NarysujPudełko(self, Condition: str | None = None) -> None:\n",
    "        \"\"\"Ta funkcja rysuje wykresy pudełkowe dla zmiennych ciągłych.\n",
    "        1) Dataset - oryginalny zestaw danych.\n",
    "        2) FloatFeatures - zmienne numeryczne\"\"\"\n",
    "\n",
    "        for float_feature in self.Num_features:\n",
    "            figure = plt.figure(num = f\"BOX_plot_{float_feature}\")\n",
    "            axes = figure.add_subplot()\n",
    "\n",
    "            sns.boxplot(self.Dataset, x = float_feature, hue = Condition)\n",
    "\n",
    "            axes.set_title(f\"Wykres pudełkowy dla zmiennej {float_feature}\")\n",
    "\n",
    "\n",
    "    def NarysujSkrzypce(self,Condition:str | None = None) -> None:\n",
    "        \"\"\"Ta funkcja rysuje wykresy skrzypcowe dla zmiennych ciągłych.\n",
    "        1) Dataset - oryginalny zestaw danych.\n",
    "        2) FloatFeatures - zmienne numeryczne\n",
    "        3) Condition = Pewna zmienna kategoryczna, za pomocą której stworzą się warunkowe wykresy skrzypcowe ze względu przynależność do klasy.\"\"\"\n",
    "\n",
    "        for float_feature in self.Num_features:\n",
    "            figure = plt.figure(num = f\"VIOLIN_plot_{float_feature}\")\n",
    "            axes = figure.add_subplot()\n",
    "\n",
    "            sns.violinplot(self.Dataset, x = float_feature, hue = Condition)\n",
    "\n",
    "            axes.set_title(f\"Wykres skrzypcowy dla zmiennej {float_feature}\")\n",
    "\n",
    "            axes.legend([])\n",
    "\n",
    "            axes.grid(True, alpha = 0.6)\n",
    "            axes.spines[['top','right']].set_visible(False)\n",
    "\n",
    "\n",
    "    def NarysujWykresParowy(self,   Condition: str | None = None) -> None:\n",
    "        \"\"\"Funkcja rysuje wykres parowy dla wszystkich par zmiennych ciągłych.\n",
    "        \"\"\"\n",
    "        sns.pairplot(self.Dataset, hue = Condition ,diag_kind = \"kde\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def Discretize(self) -> None:\n",
    "        \"\"\"Dyskretyzacja zmiennej docelowej.\"\"\"\n",
    "        labels:list[int] = [i for i in range(len(self.bins) -1)]\n",
    "     \n",
    "\n",
    "        discretized_feature:pd.Series = pd.cut(x = self.Dataset[self.target_var], \n",
    "                                    bins = self.bins, \n",
    "                                    labels = labels)\n",
    "        \n",
    "\n",
    "        self.Dataset[self.target_var_discr] = discretized_feature\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def StatystykaOpisowaZmiennych(self) -> None:\n",
    "        \"\"\"Metoda przedstawia pewne statystyki dla zmiennych, które pozwolą nam dokonać wyboru predyktorów.\n",
    "        Opis argumentów:\n",
    "        ---------\n",
    "        show_plots: bool - Czy mamy pokazywać wykresy?\n",
    "        ---------\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Agregacja rzadkich klas.\n",
    "        RareClassAggregator_inst = RareClassAggregator(q = 0.15) #Zdefiniuj obiekt klasy RareClassAggregator, który będzie agregował rzadkie klasy każdej cechy.\n",
    "\n",
    "        RareClassAggregator_inst.fit(X = self.Dataset, y = None,  #Znajdź odpowiednie parametry  estymatora.\n",
    "                                     cat_features = self.Cat_features)\n",
    "\n",
    "        self.Dataset:pd.DataFrame = RareClassAggregator_inst.transform(X = self.Dataset, cat_features = self.Cat_features) #Przekształć obecny zbiór danych.\n",
    "\n",
    "        cross_tabs:list[pd.Series] = RareClassAggregator_inst.cross_tabs #To jest lista zawierająca tablice krzyżowe każdej cechy\n",
    "\n",
    "        for cat_feature in self.Cat_features:\n",
    "            cros_tab:pd.Series = cross_tabs[cat_feature]\n",
    "\n",
    "\n",
    "            if self.show_plots is True:\n",
    "                self.PlotBarPlot(FreqTable = cros_tab, cat_feature = cat_feature, Showxlabels = False)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "        #Kasowanie zbędnej obserwacji oraz quasi-id kolumny\n",
    "        self.DeleteFutileColsAndObs()\n",
    "\n",
    "        #Nadaj zdyskretyzowanej zmiennej docelowej nazwę.\n",
    "        self.target_var_discr = self.target_var +\"_disc\"\n",
    "\n",
    "        #Zdyskretyzuj zmienną objaśnianą.\n",
    "        self.Discretize()\n",
    "\n",
    "        \n",
    "        #Narysuj wykresy charakteryzujące (relacje między zmiennymi) oraz (charakterystyki zmiennych).\n",
    "        #RYSOWANIE MACIERZY KORELACJI DLA ZMIENNYCH NUMERYCZNYCH\n",
    "\n",
    "        target_var_discr_histogram:pd.Series = self.Dataset[self.target_var_discr].value_counts(normalize = True, sort = False)\n",
    "\n",
    "        if self.show_plots is True:\n",
    "            self.ComputeAndDrawCorrelationMatrix()\n",
    "\n",
    "            #RYSOWANIE WYKRESÓW SKRZYPCOWYCH DLA ZMIENNYCH NUMERYCZNYCH\n",
    "            self.NarysujSkrzypce()\n",
    "\n",
    "            #Rysowanie wykresu parowego dla zmiennych numerycznych\n",
    "            self.NarysujWykresParowy()\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            self.PlotBarPlot(FreqTable = target_var_discr_histogram,   #Narysuj wykres słupkowy częstotliwości dla zmiennej celu zdyskretyzowanej.\n",
    "                            cat_feature =  self.target_var_discr, \n",
    "                            Showxlabels = False)\n",
    "            \n",
    "        \n",
    "            self.NarysujGęstości(Condition = self.target_var_discr)  #Wykresy gęstości warunkowe\n",
    "            self.NarysujPudełko(Condition = self.target_var_discr) #Wykresy pudełkowe warunkowe\n",
    "\n",
    "        \n",
    "            self.NarysujWykresParowy(Condition = self.target_var_discr)\n",
    "\n",
    "\n",
    "        #Ustal ostateczny zbiór predyktorów.\n",
    "        Predictors = ['Make', \"Vehicle Class\",'Engine Size(L)','Cylinders','Transmission','Fuel Type',\"Fuel Consumption City (L/100 km)\", \"Fuel Consumption Hwy (L/100 km)\",\n",
    "                                                                                       \"Fuel Consumption Comb (L/100 km)\",\"Fuel Consumption Comb (mpg)\"]\n",
    "      \n",
    "        #Podziel zbiór predyktorów na zmienne numeryczne oraz zmienne kategoryczne odpowiednio.\n",
    "        self.Num_predictors:list[str] = [feature for feature in Predictors if self.dtypes[feature] in  [np.float64, np.int16]]\n",
    "        self.Cat_predictors:list[str] = [feature for feature in Predictors if self.dtypes[feature] == \"string\"]\n",
    "\n",
    "\n",
    "        self.PCA_predictors = [\"Fuel Consumption City (L/100 km)\", \"Fuel Consumption Hwy (L/100 km)\", \n",
    "                                                                              \"Fuel Consumption Comb (L/100 km)\", \"Fuel Consumption Comb (mpg)\"]\n",
    "        \n",
    "\n",
    "        self.unique_categories = [self.Dataset[cat_var].unique() for cat_var in self.Cat_predictors]\n",
    "       \n",
    "\n",
    "    def ComputeMetric(self, TrueVSPrediction:pd.DataFrame, metric) -> pd.DataFrame:\n",
    "        \"\"\"Funkcja wylicza metrykę dokładności, która stanowi podstawę oceny modeli.\n",
    "        Opis argumentów:\n",
    "        ---------\n",
    "        TrueVSPrediction:pd.DataFrame - Ramka danych zawierająca etykiety prawdziwe oraz etykiety przewidziane. Kolumny tej ramki są indeksowane za pomocą trójki (model_name, split_indx, y_type)\n",
    "\n",
    "        ---------\n",
    "\n",
    "        Co funkcja zwraca?\n",
    "        Ramke danych typu pandas wymiaru (self.n_splits,  len(self.Models)), w której  elementy (i, j) odpowiada wartości metryki dokładności dla i-tego podziału i j-tego modelu.\n",
    "        \"\"\"\n",
    "        Models_name: list[str] = list(self.Models.keys())\n",
    "\n",
    "        MetricComparison = np.zeros(shape = [self.n_splits, len(Models_name)], \n",
    "                                    dtype = np.float64)\n",
    "        \n",
    "        for model_indx, model in enumerate(Models_name):\n",
    "            for i in range(self.n_splits):\n",
    "                y_true = TrueVSPrediction[(model, i, \"True\")]\n",
    "                y_pred = TrueVSPrediction[(model, i, \"Pred\")]\n",
    "\n",
    "                perfomance_metric =  metric(y_true = y_true,\n",
    "                                                    y_pred = y_pred, \n",
    "                                                    average = \"weighted\",\n",
    "                                                    zero_division = 0)\n",
    "\n",
    "                MetricComparison[i, model_indx] = perfomance_metric\n",
    "\n",
    "\n",
    "        return pd.DataFrame(data = MetricComparison, \n",
    "                                        columns = list(self.Models.keys()))\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    def PlotConfussionMatrix(self, type:str = \"bez strojenia\") -> None:\n",
    "        \"\"\"Metoda tworzy macierz pomyłek, którą następnie przedstawia na obrazku jako macierz pomyłek\n",
    "        Opis argumentów:\n",
    "        type:str - Typ dopasowania (ze strojeniem lub bez strojenia.)\n",
    "        ---------\n",
    "        \"\"\"\n",
    "        for model_name in self.Models.keys():\n",
    "            if type == \"bez strojenia\":\n",
    "                y_true:pd.Series = self.FactVsPrediction_untuned[(model_name, 0,  \"True\")]\n",
    "                y_predicted:pd.Series = self.FactVsPrediction_untuned[(model_name, 0,  \"Pred\")]\n",
    "\n",
    "            elif type == \"ze strojeniem\":\n",
    "                y_true:pd.Series = self.FactVsPrediction_tuned[(model_name, 0,  \"True\")]\n",
    "                y_predicted:pd.Series = self.FactVsPrediction_tuned[(model_name, 0,  \"Pred\")]\n",
    "\n",
    "            elif type == \"z wyborem cech\":\n",
    "                y_true:pd.Series = self.FactVsPrediction_FS[(model_name, 0,  \"True\")]\n",
    "                y_predicted:pd.Series = self.FactVsPrediction_FS[(model_name, 0,  \"Pred\")]\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Nieprawidłowy rodzaj trenowania. Trenuje się albo bez strojenia albo ze szkoleniem\")\n",
    "            \n",
    "\n",
    "\n",
    "            axes = plt.figure(num = f\"{model_name}_conf_matrix_{type}\").add_subplot()\n",
    "\n",
    "\n",
    "            ConfusionMatrixDisplay.from_predictions(y_true = y_true, y_pred = y_predicted, ax = axes, normalize = \"true\")\n",
    "            axes.set_title(f\"ConfMatrix dla  modelu {model_name}, wersja {type}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def PlotScoresCollectively(self, perfomance_df: pd.DataFrame, metric_name:str, type:str = \"bez strojenia\") -> None:\n",
    "        \"\"\"Metoda na wykresie przedstawia wartości metryki 'metric_name' dla różnych modeli niestrojonych lub strojonych dla różnych podziałów.\n",
    "        Opis argumentów:\n",
    "        ---------\n",
    "        perfomance_df:pd.DataFrame - Ramka danych zawierająca wartości miary dokładności w 'split_indx' podziale dla modelu 'model_name'\n",
    "        metric_name:str - Nazwa metryki dokładności.\n",
    "        type:str - Typ dopasowania (strojony lub niestrojony) \n",
    "        ---------\n",
    "        \"\"\"\n",
    "\n",
    "        figure = plt.figure()\n",
    "        axes = figure.add_subplot()\n",
    "            \n",
    "        x_values:list[int] = list(range(0, self.n_splits))\n",
    "\n",
    "        \n",
    "        for model_name in self.Models.keys():\n",
    "            axes.plot(x_values, \n",
    "                    perfomance_df[model_name])\n",
    "            \n",
    "        #Tablica, która przechowuje mediane wartości metryki dokładności w i-tym podziale.\n",
    "        averages:list[float] = []\n",
    "            \n",
    "        for i in range(self.n_splits):\n",
    "            averages.append(perfomance_df.iloc[i, :].median())\n",
    "            \n",
    "        axes.plot(x_values, averages, linestyle = \"dashed\", linewidth = 2)\n",
    "\n",
    "        axes.legend(list(self.Models.keys()) +[\"MetricMedian\"])\n",
    "        axes.set_title(f\"Dynamika zmian metryki {metric_name} dla modeli, wersja {type}\")\n",
    "\n",
    "        axes.set_xlabel(\"Numer iteracji\") #Ustaw ładną etykietkę osi Ox\n",
    "        axes.set_ylabel(f\"{metric_name}\") #Ustaw ładną etykietkę osi Oy\n",
    "\n",
    "        axes.set_xticks(x_values) #Ustaw wartości tyknięć na osi Ox\n",
    "            \n",
    "        axes.grid(True) #Dodaj linie siatki\n",
    "\n",
    "        axes.spines[['top','right']].set_visible(False) #Usuń kreskę górną oraz dolną.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def PorównajZIBeZStrojeniaModel(self, Metric_comparison_untuned:pd.DataFrame, Metric_comparison_tuned:pd.DataFrame, Metric_comparison_fs:pd.DataFrame, metric_name:str,\n",
    "                    ) -> None:\n",
    "        \"\"\"Metoda porównuje wartości miary dokładności dla każdego modelu indywidualnie w wersji bez strojenia i ze strojeniem.\n",
    "\n",
    "        Opis argumentów:\n",
    "        ---------\n",
    "        Metric_comparison_untuned:pd.DataFrame - Ramka danych, która w  komórce [ ('model_name'), ('split_indx')] zawiera wartość miary dokładności dla niestrojonego modelu 'model_name' w 'split_indx' podziale.\n",
    "        Metric_comparison_tuned:pd.DataFrame - Ramka danych, która w  komórce [ ('model_name'), ('split_indx')] zawiera wartość miary dokładności dla strojonego modelu 'model_name' w 'split_indx' podziale.\n",
    "        ---------\n",
    "        \"\"\"\n",
    "      \n",
    "        models_name: list[str] = list(self.Models.keys()) #Znajdź listę nazw modeli.\n",
    "\n",
    "    \n",
    "\n",
    "        for model_name in models_name:\n",
    "            #Stwórz okienko, na którym będzie wyświetlane porównanie między niedostrojonym, a dostrojonym modelem.\n",
    "            okno: plt.figure = plt.figure( num = f\"{model_name} (un)tuned metric comparison for {metric_name}\")\n",
    "            #Stwórz osie.\n",
    "            osie = okno.add_subplot()\n",
    "\n",
    "        \n",
    "            M_untuned:pd.Series = Metric_comparison_untuned[model_name] \n",
    "            M_tuned: pd.Series = Metric_comparison_tuned[model_name]\n",
    "            M_fs:pd.Series = Metric_comparison_fs[model_name]\n",
    "\n",
    "\n",
    "            osie.boxplot(x = [M_untuned, M_tuned, M_fs], positions = [0, 2, 4])\n",
    "\n",
    "            osie.legend([\"Niedostrojony\",'Dostrojony'])\n",
    "            osie.grid(True)\n",
    "            osie.spines[['top','right']].set_visible(False)\n",
    "            osie.set_title(f\"Porównanie niestrojonego i strojonego  {model_name}, metryka {metric_name}\")\n",
    "\n",
    "\n",
    "            osie.set_ylabel(f\"Wartość metryki {metric_name}\")\n",
    "         \n",
    "\n",
    "\n",
    "    def PorównajCzasyTrenowania(self, type:str = \"bez strojenia\"):\n",
    "        \"\"\"Ta metoda, dla ustalonego, jednego z trzech, trybów trenowania, wyświetla czasy trenowania różncych algorytmów klasyfikujących.\"\"\"\n",
    "\n",
    "        x_axis_values:list[int] = list(range(1, self.n_splits+1)) #Wartości na osi Ox.\n",
    "\n",
    "        figure  = plt.figure(num = f\"Porównanie czasów trenowania, wersja {type}\") #Zdefiniuj okno.\n",
    "        axes = figure.add_subplot() #Zdefiniuj osie.\n",
    "\n",
    "\n",
    "        if type == \"bez strojenia\":\n",
    "            for model_name in self.Models.keys():\n",
    "                axes.plot(x_axis_values, self.Untuned_train_time[model_name])\n",
    "\n",
    "        elif type == \"ze strojeniem\":\n",
    "            for model_name in self.Models.keys():\n",
    "                axes.plot(x_axis_values,  self.Tuned_train_time[model_name])\n",
    "\n",
    "        elif type == \"z wyborem cech\":\n",
    "            for model_name in self.Models.keys():\n",
    "                axes.plot(x_axis_values, self.FS_train_time[model_name])\n",
    "        else:\n",
    "            raise ValueError(\"Błędny tryb trenowania\")\n",
    "\n",
    "\n",
    "        axes.legend(list(self.Models.keys()))\n",
    "        axes.grid(True)\n",
    "        axes.spines[[\"top\", \"right\"]].set_visible(False)\n",
    "\n",
    "        axes.set_xlabel(\"Numer podziału\")\n",
    "        axes.set_ylabel('Czas tren.')\n",
    "        axes.set_title(f\"Czas trenowania modeli w milisekundach, wersja {type}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "  \n",
    "    def StworzRamkePorownawcza(self,) -> pd.DataFrame:\n",
    "        \"\"\"Funkcja stwarza tabelkę porównawcza, która jest po prostu ramką pandas. Indeksy kolumn są trzypoziomowe. Na najwyższym poziomie jest nazwa modelu, niżej jest\n",
    "        numer podziału, a na końcu tyb etykiet (prawdziwe etykiety lub przewidywane etykiety).  \\n \n",
    "        \"\"\"\n",
    "\n",
    "        Indeces = pd.MultiIndex.from_product( [list(self.Models.keys()), range(self.n_splits), [\"True\", \"Pred\"] ] #Stwórz  hierarchiczny system indeksów dla kolumn.\n",
    "                                            ,names = [\"Model\", \"iter_no\", \"y_type\"]) #Nadaj poszczególnym poziomom wyjaśnialne i sensowne nazwy.\n",
    "\n",
    "        return  pd.DataFrame(data = None,  columns = Indeces, dtype = np.int16)\n",
    "\n",
    "\n",
    "    def PrzygotujRamkiPorownawcze(self,) -> None:\n",
    "        \"\"\"Ta metoda dla każdego rodzaju modeli (niestrojony, strojony, strojony+fs) tworzy ramki porównawcze.\"\"\"\n",
    "\n",
    "        self.FactVsPrediction_untuned:pd.DataFrame =  self.StworzRamkePorownawcza()\n",
    "        self.FactVsPrediction_tuned:pd.DataFrame = self.StworzRamkePorownawcza()\n",
    "        self.FactVsPrediction_FS:pd.DataFrame = self.StworzRamkePorownawcza()\n",
    "\n",
    "\n",
    "    \n",
    "    def StworzRamkeCzasowa(self, ) -> pd.DataFrame:\n",
    "        \"\"\"\"Funkcja tworzy ramkę danych typu pandas, której wartości za indeksowane za pomocą pary (model_name, split_indx).\n",
    "        W komórce (model_name, split_indx) znajduje się czas wytrenowania modelu model_name w podziale nr split_indx\n",
    "        \"\"\"\n",
    "        \n",
    "        Col_indeces:list[str] = list(self.Models.keys())\n",
    "        Row_indeces:list[int] = list(range(self.n_splits))\n",
    "\n",
    "        return pd.DataFrame(columns = Col_indeces, index = Row_indeces, dtype = np.float64)\n",
    "    \n",
    "\n",
    "    def PrzygotujRamkiCzasowe(self, ) -> None:\n",
    "        \"\"\"Metoda definiuje trzy ramki czasowe, w których będą przechowywane czasy dopasowania modeli strojonych i niestrojonych\"\"\"\n",
    "        self.Untuned_train_time = self.StworzRamkeCzasowa()\n",
    "        self.Tuned_train_time = self.StworzRamkeCzasowa()\n",
    "        self.FS_train_time = self.StworzRamkeCzasowa()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def koduj_kategoryczne(self, X:pd.DataFrame, optymalne_cechy_kategoryczne:list[str])  -> pd.DataFrame:\n",
    "        unique_categories:list[list[str]] = [X[cat_var].unique() for cat_var in optymalne_cechy_kategoryczne]\n",
    "        \n",
    "        Koder = ColumnTransformer(transformers = [(\"OHE_opt_categ\", OneHotEncoder(categories = unique_categories,sparse_output = False), optymalne_cechy_kategoryczne)],\n",
    "                                  remainder = \"passthrough\", verbose_feature_names_out = False)\n",
    "        \n",
    "        Koder.set_output(transform = \"pandas\")\n",
    "\n",
    "        return Koder\n",
    "    \n",
    "\n",
    "    \n",
    "    def TrenujTestujAutomatycznie(self, train_indx:np.ndarray, test_indx:np.ndarray,split_indx:int = 0) -> None:\n",
    "        \"\"\"Ta metoda trenuje i testuje modele uczenia maszynowego za pomocą ręcznie dobranych predyktorów.\n",
    "        Opis argumentów:\n",
    "        train_indx:np.ndarray - Zbiór indeksów treningowych\n",
    "        test_indx: np.ndarray - Zbiór indeksów testowych.\n",
    "        cat_predictors:list[str] - Własny zbiór predyktorów kategorycznych.\n",
    "        split_indx:int - Porządkowy numer podziału.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        predictors_transformer = self.transformuj_predyktory(X = self.X, num_predictors = self.Num_predictors, #Zmienne kategoryczne tutaj nie są kodowane OHE. Są wciąż \"surowe\".\n",
    "                                                              PCA_predictors = self.PCA_predictors, cat_predictors =  self.Cat_predictors, include_OHE = False)\n",
    "        X = self.X.copy()\n",
    "\n",
    "        for cat_var in self.Cat_predictors:\n",
    "            X[cat_var] = LabelEncoder().fit_transform(y = X[cat_var])\n",
    "        \n",
    "        print(X)\n",
    "\n",
    "        # X_train:pd.DataFrame = self.X.iloc[train_indx, :] #Treningowy zbiór predyktorów.\n",
    "        # X_test:pd.DataFrame = self.X.iloc[test_indx, :] #Testowy zbiór predyktorów.\n",
    "\n",
    "\n",
    "\n",
    "        # X_train:pd.DataFrame = predictors_transformer.fit_transform(X = X_train)\n",
    "        # X_test:pd.DataFrame = predictors_transformer.transform(X = X_test)\n",
    "\n",
    "      \n",
    "\n",
    "        # for model_name in self.Models.keys():\n",
    "        #     model: 'estimator' = self.Models[model_name] #Instancja danego modelu.\n",
    "           \n",
    "\n",
    "        #     scoring = make_scorer(r2_score) if model_name == \"RegresjaLiniowa\" else make_scorer(accuracy_score)\n",
    "         \n",
    "        #     #Automatyczny wybór cech, bez tuningu.\n",
    "        #     SeqFeatSel = WrappedSequentialFeatureSelection(cat_vars = self.Cat_predictors, estimator = model, \n",
    "        #                                                    n_features_to_select = 3,\n",
    "        #                                                    direction = \"forward\", \n",
    "        #                      scoring =scoring, cv = 5)\n",
    "            \n",
    "\n",
    "        #     if model_name != \"RegresjaLiniowa\":\n",
    "        #         y_train:np.ndarray = self.y[train_indx]\n",
    "        #         y_test:np.ndarray = self.y[test_indx]\n",
    "\n",
    "            \n",
    "        #     else:\n",
    "        #         y:np.ndarray = OneHotEncoder(sparse_output = False).fit_transform(X = self.y.reshape(-1,1))\n",
    "\n",
    "        #         y_train:np.ndarray = y[train_indx, :]\n",
    "        #         y_test:np.ndarray = self.y[test_indx]\n",
    "        \n",
    "\n",
    "        #     SeqFeatSel.fit(X = X_train, y= y_train)\n",
    "\n",
    "\n",
    "        #     Optymalne_cechy:list[str] = X_train.columns[SeqFeatSel.get_support(indices = False)]\n",
    "        #     optymalne_cechy_cat:list[str] = np.intersect1d(ar1 = Optymalne_cechy, ar2 = self.Cat_predictors)\n",
    "\n",
    "        #     pretraining_coder: ColumnTransformer = self.koduj_kategoryczne(X = self.X, \n",
    "        #                                                                    optymalne_cechy_kategoryczne = optymalne_cechy_cat)\n",
    "            \n",
    "        #     pretraining_coder.fit(X = X_train[Optymalne_cechy])\n",
    "\n",
    "        #     X_train_encoded = pretraining_coder.transform(X = X_train[Optymalne_cechy])\n",
    "        #     X_test_encoded = pretraining_coder.transform(X = X_test[Optymalne_cechy])\n",
    "\n",
    "        \n",
    "            \n",
    "            \n",
    "        #     model.fit(X = X_train_encoded, y = y_train)\n",
    "\n",
    "        #     y_pred:np.ndarray = model.predict(X = X_test_encoded)\n",
    "\n",
    "        #     if model_name == \"RegresjaLiniowa\":\n",
    "        #         y_pred:np.ndarray = y_pred.argmax(axis = 1)\n",
    "                \n",
    "           \n",
    "        #     self.FactVsPrediction_FS[(model_name, split_indx, \"True\")] = y_test\n",
    "        #     self.FactVsPrediction_FS[(model_name, split_indx, \"Pred\")] = y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    def transformuj_predyktory(self, X:pd.DataFrame, num_predictors:list[str|int], PCA_predictors:list[str|int], cat_predictors:list[str|int]\n",
    "                               , include_OHE: bool = True) ->  ColumnTransformer:\n",
    "        \"\"\"Funkcja aplikuje transformacje na predyktorach numerycznych (skalowanie standardowe, analiza składowych głównych)\n",
    "        Opis argumentów: \\n\n",
    "        X:pd.DataFrame - oryginalny zbiór danych z predyktorami.\\n\n",
    "        Nun_predictors:list[str] - Lista predyktoró numerycznych. \\n\n",
    "        PCA_predictors:list[str] - Lista predyktorów dla których ma zostać wykonana analiza składowych głównych.\n",
    "        Co funkcja zwraca:\n",
    "        Przekształcony zbiór predyktorów oraz wytrenowany transformator.\n",
    "\n",
    "        \"\"\"\n",
    "        noPCA_predictors:list[str] = np.setdiff1d(ar1 = num_predictors, ar2 = PCA_predictors) #Predyktory numeryczne, które nie będą poddane PCA.\n",
    "        noPCA_transformer = Pipeline(steps = [(\"Scaler\", StandardScaler())]) #Transformer  predyktorów, które nie będą poddane PCA.\n",
    "        \n",
    "\n",
    "\n",
    "                                               \n",
    "        PCA_transformer = Pipeline(steps = [(\"Scaler\", StandardScaler()),   (\"PCA\", PCA(n_components =0.9))      ]) #Zdefiniuj rurociąg dla  predyktorów, które poddane będą PCA.\n",
    "\n",
    "\n",
    "\n",
    "        if include_OHE:\n",
    "            OHE_encoder = OneHotEncoder(categories = self.unique_categories, sparse_output = False)\n",
    "\n",
    "            Predictors_transformer = ColumnTransformer(transformers = [(\"OHE\", OHE_encoder, cat_predictors),\n",
    "                                                                        (\"Numerical\", noPCA_transformer, noPCA_predictors),\n",
    "                                                                        (\"PCA\", PCA_transformer, PCA_predictors)],\n",
    "                                                                        verbose_feature_names_out = False,)\n",
    "        else:\n",
    "            Predictors_transformer = ColumnTransformer(transformers = [(\"Numerical\", noPCA_transformer, noPCA_predictors),\n",
    "                                                                        (\"PCA\", PCA_transformer, PCA_predictors)],\n",
    "                                                                        verbose_feature_names_out = False,)\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "        Predictors_transformer.set_output(transform = \"pandas\")\n",
    "\n",
    "        return Predictors_transformer\n",
    "\n",
    "\n",
    "\n",
    "    def train_test_without_FS(self, train_indx:np.ndarray, test_indx:np.ndarray, split_indx:int = 0) -> None:\n",
    "        \"\"\"Ta metoda trenuje i testuje modele uczenia maszynowego za pomocą ręcznie dobranych predyktorów w wersji niestrojonej i strojonej.\n",
    "        Opis argumentów.\n",
    "        train_indx:np.ndarray - Zbiór indeksów treningowych\n",
    "        test_indx: np.ndarray - Zbiór indeksów testowych.\n",
    "        split_indx:int - Indykator podziału.\n",
    "        \"\"\"\n",
    "        predictors_transformer = self.transformuj_predyktory(X = self.X, \n",
    "                                                                        num_predictors = self.Num_predictors, \n",
    "                                                                        PCA_predictors = self.PCA_predictors, \n",
    "                                                                        cat_predictors = self.Cat_predictors)\n",
    "        \n",
    "  \n",
    "\n",
    "        X_train:pd.DataFrame = self.X.iloc[train_indx, :] #Treningowy zbiór predyktorów.\n",
    "        X_test:pd.DataFrame = self.X.iloc[test_indx, :] #Testowy zbiór predyktorów.\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        for model_name in self.Models.keys():\n",
    "            model: 'estimator' = self.Models[model_name] #Instancja danego modelu.\n",
    "\n",
    "            model_paramgrid: dict[str, dict] = self.Models_hipparams[model_name] #Siatka hiperparametrów modelu.\n",
    "\n",
    "\n",
    "            trans_model = Pipeline(steps = [(\"Transformations\", predictors_transformer), (\"Classifier\",model)]\n",
    "                                       )\n",
    "                 \n",
    "            #Trenowanie modeli bez strojenia hiperparametrów.\n",
    "            if model_name != \"RegresjaLiniowa\":  \n",
    "                scoring = make_scorer(accuracy_score)\n",
    "                y_train: np.ndarray = self.y[train_indx]\n",
    "                y_test:np.ndarray = self.y[test_indx]\n",
    "\n",
    "\n",
    "            else:       \n",
    "                scoring = make_scorer(r2_score)\n",
    "\n",
    "                y = OneHotEncoder(sparse_output = False).fit_transform(X = self.y.reshape(-1,1))\n",
    "\n",
    "                y_train:np.ndarray = y[train_indx]\n",
    "                y_test:np.ndarray = self.y[test_indx]\n",
    "\n",
    "                \n",
    "            trans_model.fit(X = X_train, y = y_train)\n",
    "\n",
    "\n",
    "            y_pred:np.ndarray = trans_model.predict(X = X_test)\n",
    "\n",
    "            if model_name == \"RegresjaLiniowa\":\n",
    "                y_pred:np.ndarray = y_pred.argmax(axis = 1)\n",
    "\n",
    "\n",
    "            self.FactVsPrediction_untuned[(model_name, split_indx, \"True\")] = y_test\n",
    "            self.FactVsPrediction_untuned[(model_name, split_indx, \"Pred\")] = y_pred\n",
    "\n",
    "\n",
    "\n",
    "            #Trenowanie modeli ze strojeniem hiperparametrów.\n",
    "            trans_model_paramgrid = {f\"Classifier__{param}\":model_paramgrid[param] for param in  model_paramgrid.keys()} #Dopasuj nazwy hiperparametrów do estymatora, \n",
    "                                                                                         # który nie jest bezpośrednio klasyfikatorem.\n",
    "                \n",
    "            GridSearch = GridSearchCV(estimator = trans_model, param_grid = trans_model_paramgrid, scoring = scoring, \n",
    "                                      cv = 2, )\n",
    "            \n",
    "            GridSearch.fit(X = X_train, y = y_train)\n",
    "            y_pred:np.ndarray = GridSearch.predict(X = X_test)\n",
    "\n",
    "            if model_name == \"RegresjaLiniowa\":\n",
    "                y_pred:np.ndarray = y_pred.argmax(axis = 1)\n",
    "\n",
    "            \n",
    "            self.FactVsPrediction_tuned[(model_name, split_indx, \"True\")] = y_test\n",
    "            self.FactVsPrediction_tuned[(model_name, split_indx, \"Pred\")] = y_pred\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    def PodzielZbiorDanych(self):\n",
    "        \"\"\"\"Funkcja wydobywa indeksy treningowe i indeksy testowe w n_splits podziałach. Te indeksy następnie przekazuje do metody TrenujTestuj\"\"\"\n",
    "        #Zdefiniuj stratyfikowany podział szufladkowy.\n",
    "        SSS_inst: StratifiedShuffleSplit = StratifiedShuffleSplit(n_splits = self.n_splits, \n",
    "                                                                  test_size = self.test_size, \n",
    "                                                                  train_size = self.train_size)\n",
    "        \n",
    "        self.X:pd.DataFrame = self.Dataset[ self.Cat_predictors + self.Num_predictors] #Ramka danych zawierająca predyktory, które nie zostały poddane obróbce wstępnej.\n",
    "        self.y:np.ndarray = self.Dataset[self.target_var_discr].to_numpy() #Zdyskretyzowana zmienna docelowa.\n",
    "\n",
    "\n",
    "        split_indx:int  #Zmienna, która wskazuje na numer powtórzenia pętli.\n",
    "        indx_tup: tuple[np.ndarray, np.ndarray] #Dwuelementowa krotka, która zawiera indeksy treningowe oraz indeksy testowe.\n",
    "\n",
    "        for split_indx, indx_tup in enumerate(SSS_inst.split(X = self.X, y = self.y)):\n",
    "            train_indx:np.ndarray #Indeksy treningowe. \n",
    "            test_indx:np.ndarray  #Indeksy testowe.\n",
    "\n",
    "            train_indx, test_indx = indx_tup #Wypakuj krotkę z indeksami.\n",
    "          \n",
    "            self.train_test_without_FS( train_indx = train_indx, test_indx = test_indx, split_indx = split_indx) #Wywołaj metodę do trenowania i testowania.\n",
    "            self.TrenujTestujAutomatycznie( train_indx = train_indx, test_indx = test_indx, split_indx = split_indx) #Wywołaj metodę do trenowania i testowania.\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def PorównajModele(self) -> None:\n",
    "        \"\"\"Metoda wylicza, na podstawie przewidzianych przez modele etykiet, miary dokładności modelu, takie jak: accuracy_score, f1_score, precision_score, recall score.\n",
    "        Następnie wyniki tych metryk przedstawia na wykresach.\"\"\"\n",
    "        #Zdefiniuj różne miary dokładności modeli.\n",
    "        Miary = {\"Precision\":precision_score,\n",
    "                \"Recall\":recall_score, \"F1\": f1_score}\n",
    "        \n",
    "\n",
    "        for miara in Miary.keys():\n",
    "\n",
    "          \n",
    "                \n",
    "            Metric_comparison_untuned:pd.DataFrame =  self.ComputeMetric(self.FactVsPrediction_untuned, metric = Miary[miara])\n",
    "            Metric_comparison_tuned:pd.DataFrame =  self.ComputeMetric(self.FactVsPrediction_tuned, metric = Miary[miara])\n",
    "            #Metric_comparison_FS:pd.DataFrame = self.ComputeMetric(self.FactVsPrediction_FS, metric = Miary[miara])\n",
    "\n",
    "            self.PlotScoresCollectively(Metric_comparison_untuned, miara,  \"bez strojenia\")\n",
    "            self.PlotScoresCollectively(Metric_comparison_tuned, miara, \"ze strojeniem\")\n",
    "            #self.PlotScoresCollectively(Metric_comparison_FS, miara, \"z wyborem cech\")\n",
    "\n",
    "\n",
    "\n",
    "            #Na sam koniec, dla każdego modelu indywidualnie, porównaj jego wersję niedostrojoną i strojoną.\n",
    "            #self.PorównajZIBeZStrojeniaModel(Metric_comparison_untuned, Metric_comparison_tuned, Metric_comparison_FS,\n",
    "            #                            metric_name = miara,)\n",
    "        \n",
    "\n",
    "\n",
    "        self.PlotConfussionMatrix(type = \"bez strojenia\") #Wylicz oraz narysuj macierz pomyłek dla statycznego strojenia hiperparametrów\n",
    "        self.PlotConfussionMatrix(type = \"ze strojeniem\") #Wylicz oraz narysuj macierz pomyłek dla dynamicznego strojenia hiperparametrów.\n",
    "       # self.PlotConfussionMatrix(type = \"z wyborem cech\")\n",
    "\n",
    "        self.PorównajCzasyTrenowania(type = \"bez strojenia\")\n",
    "        self.PorównajCzasyTrenowania(type = \"ze strojeniem\")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Make  Vehicle Class  Transmission  Fuel Type  Engine Size(L)  Cylinders  \\\n",
      "0        0              0            11          4             2.0          4   \n",
      "1        0              0            21          4             2.4          4   \n",
      "2        0              0            18          4             1.5          4   \n",
      "3        0             11            12          4             3.5          6   \n",
      "4        0             11            12          4             3.5          6   \n",
      "...    ...            ...           ...        ...             ...        ...   \n",
      "7380    35             11            14          4             2.0          4   \n",
      "7381    35             11            14          4             2.0          4   \n",
      "7382    35             11            14          4             2.0          4   \n",
      "7383    35             12            14          4             2.0          4   \n",
      "7384    35             12            14          4             2.0          4   \n",
      "\n",
      "      Fuel Consumption City (L/100 km)  Fuel Consumption Hwy (L/100 km)  \\\n",
      "0                                  9.9                              6.7   \n",
      "1                                 11.2                              7.7   \n",
      "2                                  6.0                              5.8   \n",
      "3                                 12.7                              9.1   \n",
      "4                                 12.1                              8.7   \n",
      "...                                ...                              ...   \n",
      "7380                              10.7                              7.7   \n",
      "7381                              11.2                              8.3   \n",
      "7382                              11.7                              8.6   \n",
      "7383                              11.2                              8.3   \n",
      "7384                              12.2                              8.7   \n",
      "\n",
      "      Fuel Consumption Comb (L/100 km)  Fuel Consumption Comb (mpg)  \n",
      "0                                  8.5                         33.0  \n",
      "1                                  9.6                         29.0  \n",
      "2                                  5.9                         48.0  \n",
      "3                                 11.1                         25.0  \n",
      "4                                 10.6                         27.0  \n",
      "...                                ...                          ...  \n",
      "7380                               9.4                         30.0  \n",
      "7381                               9.9                         29.0  \n",
      "7382                              10.3                         27.0  \n",
      "7383                               9.9                         29.0  \n",
      "7384                              10.7                         26.0  \n",
      "\n",
      "[7385 rows x 10 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 60\u001b[0m\n\u001b[0;32m     53\u001b[0m WielkiEstymator \u001b[38;5;241m=\u001b[39m ModelsComparisom(Filename \u001b[38;5;241m=\u001b[39m file_name, target_var \u001b[38;5;241m=\u001b[39m target_var, dtypes \u001b[38;5;241m=\u001b[39m dtypes, \n\u001b[0;32m     54\u001b[0m                                   Models \u001b[38;5;241m=\u001b[39m Models, Models_hipparams \u001b[38;5;241m=\u001b[39m Models_hipparams, \n\u001b[0;32m     55\u001b[0m                                   n_splits \u001b[38;5;241m=\u001b[39m n_splits, train_size  \u001b[38;5;241m=\u001b[39m train_size, test_size \u001b[38;5;241m=\u001b[39m test_size, \n\u001b[0;32m     56\u001b[0m                                   bins \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m150\u001b[39m, \u001b[38;5;241m250\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)], show_plots \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     57\u001b[0m                                   )\n\u001b[0;32m     59\u001b[0m WielkiEstymator\u001b[38;5;241m.\u001b[39mStatystykaOpisowaZmiennych()\n\u001b[1;32m---> 60\u001b[0m \u001b[43mWielkiEstymator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPodzielZbiorDanych\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m WielkiEstymator\u001b[38;5;241m.\u001b[39mPorównajModele()\n",
      "Cell \u001b[1;32mIn[15], line 756\u001b[0m, in \u001b[0;36mModelsComparisom.PodzielZbiorDanych\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    752\u001b[0m test_indx:np\u001b[38;5;241m.\u001b[39mndarray  \u001b[38;5;66;03m#Indeksy testowe.\u001b[39;00m\n\u001b[0;32m    754\u001b[0m train_indx, test_indx \u001b[38;5;241m=\u001b[39m indx_tup \u001b[38;5;66;03m#Wypakuj krotkę z indeksami.\u001b[39;00m\n\u001b[1;32m--> 756\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_test_without_FS\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_indx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_indx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_indx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_indx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_indx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msplit_indx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Wywołaj metodę do trenowania i testowania.\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTrenujTestujAutomatycznie( train_indx \u001b[38;5;241m=\u001b[39m train_indx, test_indx \u001b[38;5;241m=\u001b[39m test_indx, split_indx \u001b[38;5;241m=\u001b[39m split_indx)\n",
      "Cell \u001b[1;32mIn[15], line 722\u001b[0m, in \u001b[0;36mModelsComparisom.train_test_without_FS\u001b[1;34m(self, train_indx, test_indx, split_indx)\u001b[0m\n\u001b[0;32m    717\u001b[0m                                                                              \u001b[38;5;66;03m# który nie jest bezpośrednio klasyfikatorem.\u001b[39;00m\n\u001b[0;32m    719\u001b[0m GridSearch \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator \u001b[38;5;241m=\u001b[39m trans_model, param_grid \u001b[38;5;241m=\u001b[39m trans_model_paramgrid, scoring \u001b[38;5;241m=\u001b[39m scoring, \n\u001b[0;32m    720\u001b[0m                           cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, )\n\u001b[1;32m--> 722\u001b[0m \u001b[43mGridSearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    723\u001b[0m y_pred:np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m GridSearch\u001b[38;5;241m.\u001b[39mpredict(X \u001b[38;5;241m=\u001b[39m X_test)\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegresjaLiniowa\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1572\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1276\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1272\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m > 1 does not have any effect when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1273\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1274\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs))\n\u001b[0;32m   1275\u001b[0m         )\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_liblinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\pawel\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1215\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m   1212\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m   1214\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[1;32m-> 1215\u001b[0m raw_coef_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mliblinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_wrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;66;03m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;66;03m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;66;03m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;66;03m# srand supports\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m n_iter_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_iter_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dtypes = { \"Make\": \"string\", #Określ typ danych każdej cechy w ramce danych.\n",
    "            \"Model\":\"string\",\n",
    "            \"Vehicle Class\":\"string\",\n",
    "            \"Engine Size(L)\":np.float64,\n",
    "            \"Cylinders\":np.int16,\n",
    "            \"Transmission\":\"string\",\n",
    "            \"Fuel Type\":\"string\",\n",
    "            \"Fuel Consumption City (L/100 km)\":np.float64,\n",
    "            \"Fuel Consumption Hwy (L/100 km)\":np.float64,\n",
    "            \"Fuel Consumption Comb (L/100 km)\":np.float64,\n",
    "            \"Fuel Consumption Comb (mpg)\":np.float64,\n",
    "            \"CO2 Emissions(g/km)\":np.float64}\n",
    "\n",
    "\n",
    "# Definicja modeli z hiperparametrami\n",
    "Models = {\n",
    "    \"DrzewkoDecyzyjne\": DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\", min_samples_split=2), \n",
    "    \"LasLosowy\": RandomForestClassifier(n_estimators=15, criterion='gini'), \n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"RegresjaLiniowa\": MultiOutputRegressor(estimator = LinearRegression()),\n",
    "    \"RegresjaLogistyczna\": LogisticRegression()\n",
    "}                \n",
    "#Słownik do przechowywania hiperparametrów modeli:\n",
    "Models_hipparams = {\"DrzewkoDecyzyjne\":{\"criterion\":['gini','entropy'],\n",
    "                                       \"splitter\":['best','random'],\n",
    "                                       \"min_samples_split\":[2,3],\n",
    "                                        \"min_samples_leaf\":[2,3]},\n",
    "\n",
    "                    \"LasLosowy\":{\"n_estimators\":list(range(5, 25, 5)),\n",
    "                                  \"criterion\":['gini','entropy'],\n",
    "                                         \"min_samples_split\":[2,3],\n",
    "                                       \"min_samples_leaf\":[2,3]},\n",
    "                                        \n",
    "                   \"KNN\": {\"n_neighbors\":list(range(1, 10, 2)),\n",
    "                            \"p\":[1,2]},\n",
    "                            \n",
    "                        \"RegresjaLogistyczna\": {\n",
    "                            \"penalty\":['l2'],\n",
    "                              \"solver\":['liblinear','newton-cg',],\n",
    "                              \n",
    "                         }, \"RegresjaLiniowa\":{}\n",
    "                             }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "target_var: str = \"CO2 Emissions(g/km)\" #To jest nazwa zmiennej docelowej.\n",
    "file_name:str = \"CO2Emission.csv\"\n",
    "n_splits:int = 15\n",
    "train_size:float = 0.8\n",
    "test_size:float  = 1 - train_size\n",
    "\n",
    "WielkiEstymator = ModelsComparisom(Filename = file_name, target_var = target_var, dtypes = dtypes, \n",
    "                                  Models = Models, Models_hipparams = Models_hipparams, \n",
    "                                  n_splits = n_splits, train_size  = train_size, test_size = test_size, \n",
    "                                  bins = [-float('inf'), 150, 250, float('inf')], show_plots = False\n",
    "                                  )\n",
    "\n",
    "WielkiEstymator.StatystykaOpisowaZmiennych()\n",
    "WielkiEstymator.PodzielZbiorDanych()\n",
    "WielkiEstymator.PorównajModele()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pytania badawcze przykładowe:\n",
    "# 1) Jak liczba klas docelowych wpływa na skuteczność metod? Czy skuteczność modelu domyślnie maleje, jeżeli liczba klas docelowych wzrośnie?\n",
    "# 2) Jak skuteczne są metody proste w porównaniu z metodami bardziej zaawansowanymi.\n",
    "# 3) Jak istotne jest strojenie parametrów? Czy statyczne strojenie parametrów ulega dynamicznego strojeniu parametrów.\n",
    "# 4) Jak istotny jest wybór optymalnych cech? Czy należy uwzględnić wszystkie względne? A może wystarczy tylko kilka cech?\n",
    "# 5) Jak prezentuje się dokładnośc predykcji w stosunku do poszczególnych klas? Czy klasy rzadsze są łatwiej przewidywalne?\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "#Typy wyboru cech.\n",
    "#1) Filtry, czyli  kasujemy cechy, niezależnie od modelu, na podstawie statystyk.\n",
    "#2) wrappers, czyli metody, które są \"owinięte\" wokół pewnego modelu.\n",
    "#3) embedded methods.\n",
    "\n",
    "\n",
    "#Jak sobie poradzić ze zmiennymi kategorycznymi wysoko-kardynalnymi \n",
    "#1)Weź to przeczytaj: https://github.com/rasbt/mlxtend/issues/502\n",
    "\n",
    "# Najpierw ustal klasy, które są determinowane przez regulacje. Będzie ich trzy\n",
    "# Potem ustaw własne progi dla klas emisyjności, aby liczba klas wzrosła do 5.\n",
    "# Beamer w latexu do robienia prezentacji.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co dalej do roboty?:\n",
    "2) Zamknięćie wszystkich funkcji w jedną, potężna funkcję.\n",
    "4) Liczenie wskaźników dokładności, jeżeli selekcja cech była wybierana automatycznie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
